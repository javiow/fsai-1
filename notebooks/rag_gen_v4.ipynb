{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2787c6027a9610f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas tqdm transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:16.029284Z",
     "start_time": "2025-08-04T09:40:16.025099Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9fbe2f1b329d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:18.581689Z",
     "start_time": "2025-08-04T09:40:18.542863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>금융산업의 이해와 관련하여 금융투자업의 구분에 해당하지 않는 것은?\\n1 소비자금융...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>위험 관리 계획 수립 시 고려해야 할 요소로 적절하지 않은 것은?\\n1 수행인력\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>재해 복구 계획 수립 시 고려해야 할 요소로 옳지 않은 것은?\\n1 복구 절차 수립...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>트로이 목마(Trojan) 기반 원격제어 악성코드(RAT)의 특징과 주요 탐지 지표...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>TEST_510</td>\n",
       "      <td>\"정보보호최고책임자\"의 임명에 관한 설명으로 옳지 않은 것은?\\n1 정보보호최고책임...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>TEST_511</td>\n",
       "      <td>IPv6 주소 체계의 주요 특징으로 옳지 않은 것은?\\n1 NAT 필요성 감소\\n2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>TEST_512</td>\n",
       "      <td>하이브리드 위협에 대한 설명으로 가장 적절한 것은?\\n1 사이버 공간에서만 발생하는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>TEST_513</td>\n",
       "      <td>전자금융거래법의 주요 목적 중 하나는 무엇인가?\\n1 전자금융거래의 비대면성 강화\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>TEST_514</td>\n",
       "      <td>전자서명인증업무 운영기준에 포함되지 않는 항목은 무엇인가?\\n1 전자서명인증서의 유...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                           Question\n",
       "0    TEST_000  금융산업의 이해와 관련하여 금융투자업의 구분에 해당하지 않는 것은?\\n1 소비자금융...\n",
       "1    TEST_001  위험 관리 계획 수립 시 고려해야 할 요소로 적절하지 않은 것은?\\n1 수행인력\\n...\n",
       "2    TEST_002  관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\\n...\n",
       "3    TEST_003  재해 복구 계획 수립 시 고려해야 할 요소로 옳지 않은 것은?\\n1 복구 절차 수립...\n",
       "4    TEST_004  트로이 목마(Trojan) 기반 원격제어 악성코드(RAT)의 특징과 주요 탐지 지표...\n",
       "..        ...                                                ...\n",
       "510  TEST_510  \"정보보호최고책임자\"의 임명에 관한 설명으로 옳지 않은 것은?\\n1 정보보호최고책임...\n",
       "511  TEST_511  IPv6 주소 체계의 주요 특징으로 옳지 않은 것은?\\n1 NAT 필요성 감소\\n2...\n",
       "512  TEST_512  하이브리드 위협에 대한 설명으로 가장 적절한 것은?\\n1 사이버 공간에서만 발생하는...\n",
       "513  TEST_513  전자금융거래법의 주요 목적 중 하나는 무엇인가?\\n1 전자금융거래의 비대면성 강화\\...\n",
       "514  TEST_514  전자서명인증업무 운영기준에 포함되지 않는 항목은 무엇인가?\\n1 전자서명인증서의 유...\n",
       "\n",
       "[515 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4220b8fb-00e2-48f6-a4b4-65555046c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[[492, 446, 362, 111, 121, 162, 434, 4, 7, 82, 101, 102, 126, 131, 182, 458], :]\n",
    "# test = test.iloc[:20, :]1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511d0f6574e94d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:31.325190Z",
     "start_time": "2025-08-04T09:40:31.318730Z"
    }
   },
   "outputs": [],
   "source": [
    "# 객관식 여부 판단 함수\n",
    "def is_multiple_choice(question_text):\n",
    "    \"\"\"\n",
    "    객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
    "    \"\"\"\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# 질문과 선택지 분리 함수\n",
    "def extract_question_and_choices(full_text):\n",
    "    \"\"\"\n",
    "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "    \"\"\"\n",
    "    lines = full_text.strip().split(\"\\n\")\n",
    "    q_lines = []\n",
    "    options = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "            options.append(line.strip())\n",
    "        else:\n",
    "            q_lines.append(line.strip())\n",
    "\n",
    "    question = \" \".join(q_lines)\n",
    "    return question, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8936cae9e5905b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:33.019426Z",
     "start_time": "2025-08-04T09:40:33.014808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 프롬프트 생성기\n",
    "def make_prompt_auto(text):\n",
    "    if is_multiple_choice(text):\n",
    "        question, options = extract_question_and_choices(text)\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                # \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"질문: {text}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c50b9e93981919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:50:33.144854Z",
     "start_time": "2025-08-04T09:40:35.988653Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 멀티법령 RAG 전처리/인덱싱 (PIPA, 신용정보법, 전자서명법, 정보통신망법)\n",
    "\n",
    "import os, re, json, math\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "\n",
    "# ===== 사용자 환경 상수 =====\n",
    "# LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "LLM_ID = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 경량/호환성 위주\n",
    "# EMB_MODEL = \"intfloat/multilingual-e5-base\"\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 600\n",
    "TOP_K = 4\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ===== 법령 설정: 파일 경로 + 제거/정규식 패턴 =====\n",
    "LAW_CONFIG = {\n",
    "    # 개인정보 보호법\n",
    "    \"pipa\": {\n",
    "        \"law_name\": \"개인정보 보호법\",\n",
    "        \"pdf_path\": \"../data/개인정보 보호법(법률)(제19234호)(20250313).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터',\n",
    "            r'국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>',         # <개정 …>, <신설 …>\n",
    "            r'\\[[^\\]]+\\]',      # [본조신설 …]\n",
    "        ],\n",
    "    },\n",
    "    # 신용정보의 이용 및 보호에 관한 법률\n",
    "    \"ciupa\": {\n",
    "        \"law_name\": \"신용정보법\",\n",
    "        \"pdf_path\": \"../data/신용정보의 이용 및 보호에 관한 법률(법률)(제20304호)(20240814).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*신용정보.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자서명법\n",
    "    \"es_act\": {\n",
    "        \"law_name\": \"전자서명법\",\n",
    "        \"pdf_path\": \"../data/전자서명법(법률)(제18479호)(20221020).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자서명법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 정보통신망 이용촉진 및 정보보호 등에 관한 법률\n",
    "    \"icn_act\": {\n",
    "        \"law_name\": \"정보통신망법\",\n",
    "        \"pdf_path\": \"../data/정보통신망 이용촉진 및 정보보호 등에 관한 법률(법률)(제20678호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*정보통신망.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자금융거래법\n",
    "    \"eft_act\": {\n",
    "        \"law_name\": \"전자금융거래법\",\n",
    "        \"pdf_path\": \"../data/전자금융거래법(법률)(제19734호)(20240915).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자금융거래.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자금융감독규정\n",
    "    \"rs_act\": {\n",
    "        \"law_name\": \"전자금융감독규정\",\n",
    "        \"pdf_path\": \"../data/전자금융감독규정(금융위원회고시)(제2025-4호)(20250205).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자금융거래.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 자본시장법\n",
    "    \"fis_act\": {\n",
    "        \"law_name\": \"자본시장법\",\n",
    "        \"pdf_path\": \"../data/자본시장과 금융투자업에 관한 법률(법률)(제20718호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*자본시장.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897a3e9b-e2c7-49b4-92b8-adad6e0a19e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ===== 토크나이저: 토큰 길이 계산/청킹 =====\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    # 고정폭 lookbehind: '다.' 또는 일반 종결부호\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdeaf3b-c1a8-4e38-86d6-38975e2ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 공통 정제 =====\n",
    "def normalize_common(text: str) -> str:\n",
    "    # 한자 제거\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    # circled numbers → (n)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    # 공백/빈 괄호 정리\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_by_config(text: str, drop_patterns: List[str]) -> str:\n",
    "    for pat in drop_patterns:\n",
    "        text = re.sub(pat, '', text)\n",
    "    return normalize_common(text)\n",
    "\n",
    "# ===== 조문 단위 분리 =====\n",
    "ARTICLE_HEADER_PATTERN = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'  # 제X조(제목) / 제X조의Y(제목)\n",
    "\n",
    "def split_articles(raw_text: str) -> List[Tuple[str, str, str]]:\n",
    "    parts = re.split(ARTICLE_HEADER_PATTERN, raw_text)\n",
    "    out = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m:\n",
    "            continue\n",
    "        article_id = m.group(1)          # 제xx조 / 제xx조의y\n",
    "        article_title = m.group(2)       # (제목)\n",
    "        out.append((article_id, article_title, body))\n",
    "    return out\n",
    "\n",
    "def chunk_article(article_body: str, header: str) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(article_body) or [article_body]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > CHUNK_TOKENS and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            # overlap: 마지막 문장 유지\n",
    "            keep = cur[-1] if CHUNK_OVERLAP > 0 and cur else \"\"\n",
    "            cur = [keep] if keep else []\n",
    "            cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "        cur.append(s); cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c4719f-2c89-41e5-9df2-d3d19e18316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1942/3198458753.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# ===== 데이터 클래스 =====\n",
    "@dataclass\n",
    "class LawDoc:\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "# ===== 임베딩 =====\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 128,\n",
    "        \"convert_to_numpy\": True,\n",
    "        \"convert_to_tensor\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# ===== 인덱스 (FAISS HNSW) =====\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 32) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, m)\n",
    "    idx.hnsw.efSearch = ef_search\n",
    "    idx.add(vectors.astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "# ===== 파이프라인: 1) 로드 → 2) 정제 → 3) 조문분리 → 4) 청킹 → 5) 임베딩/인덱스\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        text += t + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def preprocess_law(law_id: str, cfg: Dict) -> List[LawDoc]:\n",
    "    raw = load_pdf_text(cfg[\"pdf_path\"])\n",
    "    cleaned = clean_text_by_config(raw, cfg[\"drop_patterns\"])\n",
    "    articles = split_articles(cleaned)\n",
    "\n",
    "    docs: List[LawDoc] = []\n",
    "    # 시행일 파싱(있으면): [시행 YYYY. M. D.]가 머리말에 있는 경우가 많으나 PDF마다 다름 → 필요 시 별도 파서\n",
    "    effective_date = None  # 필요 시 추출 로직 추가\n",
    "\n",
    "    for article_id, title, body in articles:\n",
    "        header = f'{cfg[\"law_name\"]} {article_id}({title})'\n",
    "        chunks = chunk_article(body, header)\n",
    "        for ch in chunks:\n",
    "            meta = {\n",
    "                \"law_id\": law_id,\n",
    "                \"law_name\": cfg[\"law_name\"],\n",
    "                \"article_id\": article_id,\n",
    "                \"article_title\": title,\n",
    "                \"effective_date\": effective_date,   # None 가능\n",
    "                \"tok_len\": token_len(ch),\n",
    "                \"source_uri\": cfg.get(\"pdf_path\"),\n",
    "                \"version\": None\n",
    "            }\n",
    "            docs.append(LawDoc(text=ch, meta=meta))\n",
    "    return docs\n",
    "\n",
    "def build_indices(all_docs: List[LawDoc]):\n",
    "    # (a) 법령별 인덱스\n",
    "    per_law_docs: Dict[str, List[LawDoc]] = {}\n",
    "    for d in all_docs:\n",
    "        per_law_docs.setdefault(d.meta[\"law_id\"], []).append(d)\n",
    "\n",
    "    indices = {}\n",
    "    for law_id, docs in per_law_docs.items():\n",
    "        mat = np.array(embeddings.embed_documents([d.text for d in docs]), dtype=np.float32)\n",
    "        indices[f\"faiss_hnsw_{law_id}\"] = {\n",
    "            \"index\": build_faiss_hnsw(mat, m=32, ef_search=32),\n",
    "            \"docs\": docs\n",
    "        }\n",
    "\n",
    "    # (b) 글로벌 인덱스\n",
    "    mat_all = np.array(embeddings.embed_documents([d.text for d in all_docs]), dtype=np.float32)\n",
    "    indices[\"faiss_hnsw_all\"] = {\n",
    "        \"index\": build_faiss_hnsw(mat_all, m=32, ef_search=32),\n",
    "        \"docs\": all_docs\n",
    "    }\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24da7e19-e130-4040-aef4-795957cd3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 개인정보 보호법 → chunks: 160\n",
      "[OK] 신용정보법 → chunks: 125\n",
      "[OK] 전자서명법 → chunks: 26\n",
      "[OK] 정보통신망법 → chunks: 113\n",
      "[OK] 전자금융거래법 → chunks: 78\n",
      "[OK] 전자금융감독규정 → chunks: 113\n",
      "[OK] 자본시장법 → chunks: 782\n",
      "[OK] built indices: ['faiss_hnsw_pipa', 'faiss_hnsw_ciupa', 'faiss_hnsw_es_act', 'faiss_hnsw_icn_act', 'faiss_hnsw_eft_act', 'faiss_hnsw_rs_act', 'faiss_hnsw_fis_act', 'faiss_hnsw_all']\n"
     ]
    }
   ],
   "source": [
    "all_docs: List[LawDoc] = []\n",
    "for law_id, cfg in LAW_CONFIG.items():\n",
    "    if not os.path.exists(cfg[\"pdf_path\"]):\n",
    "        print(f\"[WARN] PDF not found: {cfg['pdf_path']}\")\n",
    "        continue\n",
    "    docs = preprocess_law(law_id, cfg)\n",
    "    all_docs.extend(docs)\n",
    "    print(f\"[OK] {cfg['law_name']} → chunks: {len(docs)}\")\n",
    "\n",
    "indices = build_indices(all_docs)\n",
    "print(\"[OK] built indices:\", list(indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c173872-00c8-4149-8327-1836b6a312b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 라우팅 규칙 =====\n",
    "ARTICLE_PTRN = re.compile(r\"제\\d+조(?:의\\d+)?\")  # 조문 표기\n",
    "LAW_HINTS = {\n",
    "    \"pipa\": (\"개인정보\", \"개인 정보\", \"개인정보보호법\"),\n",
    "    \"ciupa\": (\"신용정보\",  \"신용정보법\"),\n",
    "    \"es_act\": (\"전자서명\", \"전자서명법\"),\n",
    "    \"icn_act\": (\"정보통신망\", \"통신망\", \"정보통신망법\"),\n",
    "    \"eft_act\": (\"전자금융\", \"금융거래\", \"전자금융거래법\"),\n",
    "    \"rs_act\": (\"전자금융감독\", \"금융감독\", \"전자금융감독규정\"),\n",
    "    \"fis_act\": (\"자본\", \"자본시장\", \"금융투자\", \"채권\", \"자본시장법\", \"금융투자업\", \"투자\"),\n",
    "}\n",
    "\n",
    "def detect_law_id(query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    for law_id, kws in LAW_HINTS.items():\n",
    "        if any(kw.lower() in q for kw in kws):\n",
    "            return law_id\n",
    "    return None\n",
    "\n",
    "def route_is_domain(query: str) -> bool:\n",
    "    # 법/금융/보안 도메인 간단 라우터 (검색 여부 판단용)\n",
    "    domain_kws = (\"법\", \"조(\", \"과징금\", \"처벌\", \"보안\", \"침해\", \"금융\", \"개인정보\", \"신용정보\", \"전자서명\", \"정보통신\", \"전자금융\", \"금융감독\", \"자본\", \"자본시장\", \"금융투자\", \"채권\", \"투자\")\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in domain_kws) or bool(ARTICLE_PTRN.search(query))\n",
    "\n",
    "def choose_index(indices: dict, query: str):\n",
    "    \"\"\"\n",
    "    1) 질의에서 법령 단서 -> 해당 법 인덱스 우선\n",
    "    2) 조문 패턴만 있거나 단서가 불분명 -> 글로벌 인덱스\n",
    "    3) 아무 단서도 없으면 None (베이스모델 경로)\n",
    "    \"\"\"\n",
    "    law_id = detect_law_id(query)\n",
    "    if law_id:\n",
    "        key = f\"faiss_hnsw_{law_id}\"\n",
    "        if key in indices:\n",
    "            return indices[key]  # {\"index\": ..., \"docs\": ...}\n",
    "    # 법령 단서 없지만 도메인성/조문 표기는 있는 경우 글로벌\n",
    "    if route_is_domain(query) and \"faiss_hnsw_all\" in indices:\n",
    "        return indices[\"faiss_hnsw_all\"]\n",
    "    return None  # 베이스모델 직행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849912de-b930-4c92-9909-2e759fbf42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 검색 with 점수 (해당 인덱스에서) =====\n",
    "def faiss_search_with_scores_from_index(index_entry: dict, query: str, top_k: int = TOP_K):\n",
    "    # embeddings는 상위 스코프에서 로드되었다고 가정\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index_entry[\"index\"].search(qv, top_k)     # L2 거리 (정규화 벡터 가정)\n",
    "    cos = 1.0 - (D[0] / 2.0)                          # L2 -> cosine\n",
    "    out = []\n",
    "    docs = index_entry[\"docs\"]\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        ii = int(i)\n",
    "        if ii >= 0:\n",
    "            out.append((docs[ii], float(cos[idx])))\n",
    "    return out  # [(LawDoc, cosine), ...]\n",
    "\n",
    "# ===== 컨텍스트 패킹 (질문 주신 코드 재사용 + tok_len 캐시) =====\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114f810e-a409-42fb-879c-739125042131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 프롬프트 =====\n",
    "def chat_prompt(system, user):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    return llm_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "def build_prompt(query: str, use_context: bool, context) -> Tuple[str, int]:\n",
    "    \"\"\"컨텍스트 유무에 따라 간단 프롬프트와 max_len을 반환\"\"\"\n",
    "    # 객관식 질문\n",
    "    if is_multiple_choice(query):\n",
    "        question, options = extract_question_and_choices(query)\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                \"답변을 선택한 근거를 찾아 작성하세요.\"\n",
    "                f\"=== 컨텍스트 ===\\n{context}\\n=== 끝 ===\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                \"\\n근거:\"\n",
    "            )\n",
    "\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "            return prompt, max_len\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "            return prompt, max_len\n",
    "    # 주관식 질문\n",
    "    else:\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{context}\\n=== 끝 ===\\n\"\n",
    "                \"\"\"아래 질문에 대해 **사실에 근거한 간결한 답변**을 작성하세요. \n",
    "\n",
    "규칙:\n",
    "1. 답변은 2~3문장 이내로 작성합니다. 장황한 서론, 결론 문구는 쓰지 않습니다.\n",
    "2. 불확실할 경우 '알 수 없습니다'라고 답하고 생성을 종료합니다.\n",
    "3. 특수문자 없이 오로지 한글과 숫자로만 대답합니다.\n",
    "4. 답변을 작성한 근거를 찾아 작성하세요.\n",
    "\"\"\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "                \"\\n근거:\"\n",
    "            )\n",
    "\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "            return prompt, max_len\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"\"\"아래 질문에 대해 **사실에 근거한 간결한 답변**을 작성하세요. \n",
    "\n",
    "규칙:\n",
    "1. 답변은 2~3문장 이내로 작성합니다. 장황한 서론, 결론 문구는 쓰지 않습니다.\n",
    "2. 불확실할 경우 '알 수 없습니다'라고 답하고 생성을 종료합니다.\n",
    "3. 특수문자 없이 오로지 한글과 숫자로만 대답합니다.\n",
    "\"\"\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "            return prompt, max_len\n",
    "\n",
    "\n",
    "# ===== 생성 토큰 상한 =====\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 96 if opt_cnt >= 2 else 192\n",
    "\n",
    "# ===== 메인: 다중 인덱스 기반 텍스트 생성 =====\n",
    "def generate_answer_with_indices(query: str, indices: dict) -> str:\n",
    "    \"\"\"\n",
    "    indices: build_indices() 반환 구조\n",
    "    - 라우팅 → 해당 인덱스에서 top-k 검색(점수 포함)\n",
    "    - 최고 유사도 임계치 미만이면 컨텍스트 없이 베이스모델 생성(Adaptive RAG)\n",
    "    \"\"\"\n",
    "    # 0) 우선, 일반상식/비도메인은 곧장 베이스모델\n",
    "    if not route_is_domain(query):\n",
    "        prompt, max_len = build_prompt(query, use_context=False, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False,\n",
    "                                     temperature=0.2,\n",
    "                                     top_p=0.9,\n",
    "                                     repetition_penalty=1.1,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id,\n",
    "                                     )\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 1) 인덱스 선택\n",
    "    idx_entry = choose_index(indices, query)\n",
    "\n",
    "    # 2) 인덱스가 없으면 베이스모델\n",
    "    if idx_entry is None:\n",
    "        prompt, max_len = build_prompt(query, use_context=False, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False,\n",
    "                                     temperature=0.2,\n",
    "                                     top_p=0.9,\n",
    "                                     repetition_penalty=1.1,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id,\n",
    "                                     )\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 3) 선택 인덱스에서 검색 + 점수\n",
    "    scored = faiss_search_with_scores_from_index(idx_entry, query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # 4) 임계치: 충분히 유사할 때만 컨텍스트 사용 (속도 최적화)\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    context = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else None\n",
    "    prompt, max_len = build_prompt(query, use_context, context)\n",
    "    print(prompt)\n",
    "\n",
    "    # 5) LLM 생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=dynamic_max_new_tokens(query),\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "            )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70d9f6a5-b6eb-4187-9b78-ed6d262a9584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d69044a437e4b6abdcbcccdd614ac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88a1fbb90a347f9a5dbab53f78cb88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168b0b6051da4212b76b820032f109be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a466f6e9a9604cbc97311fd8d8d75198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816299d060774e8fb382c48c585c7372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Data processing error: CAS service error : IO Error: No space left on device (os error 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---------------- LLM 로드 & 생성 ----------------\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLLM_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      9\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_tf32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3263\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3276\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3280\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3283\u001b[0m ):\n\u001b[1;32m   3284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:1038\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1171\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1723\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1722\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1723\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Data processing error: CAS service error : IO Error: No space left on device (os error 28)"
     ]
    }
   ],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ba17ed873ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 함수\n",
    "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
    "    \"\"\"\n",
    "    - \"답변:\" 이후 텍스트만 추출\n",
    "    - 객관식 문제면: 정답 숫자만 추출 (실패 시 전체 텍스트 또는 기본값 반환)\n",
    "    - 주관식 문제면: 전체 텍스트 그대로 반환\n",
    "    - 공백 또는 빈 응답 방지: 최소 \"미응답\" 반환\n",
    "    \"\"\"\n",
    "    # \"답변:\" 기준으로 텍스트 분리\n",
    "    if \"답변:\" in generated_text:\n",
    "        text = generated_text.split(\"답변:\")[-1].strip()\n",
    "    else:\n",
    "        text = generated_text.strip()\n",
    "\n",
    "    # 공백 또는 빈 문자열일 경우 기본값 지정\n",
    "    if not text:\n",
    "        return \"미응답\"\n",
    "\n",
    "    # 객관식 여부 판단\n",
    "    is_mc = is_multiple_choice(original_question)\n",
    "\n",
    "    if is_mc:\n",
    "        # 숫자만 추출\n",
    "        match = re.match(r\"\\D*([1-9][0-9]?)\", text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            # 숫자 추출 실패 시 \"0\" 반환\n",
    "            return \"0\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81099eeb-cdca-4a30-9c47-c27adf337c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_explanations(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    키워드 뒤 설명을 제거하고 키워드만 남김\n",
    "    - '키워드: 설명' → '키워드'\n",
    "    - '키워드 ... 설명문' → '키워드'\n",
    "    \"\"\"\n",
    "    lines = answer.split(\"\\n\")\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # \"키워드: 설명\" → \"키워드\"\n",
    "        m = re.match(r\"^\\s*[-0-9.]*\\s*([^\\:]+)\", line)\n",
    "        if m:\n",
    "            keyword = m.group(1).strip()\n",
    "            cleaned.append(keyword)\n",
    "    return \"\\n\".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8bd6175f1111c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
    "    print(\"#################### Question ###########################\")\n",
    "    print(q)\n",
    "    ans = generate_answer_with_indices(q, indices)\n",
    "    pred_answer = extract_answer_only(ans, original_question=q)\n",
    "    print(\"#################### Answer ###########################\")\n",
    "    print(strip_explanations(pred_answer))\n",
    "    preds.append(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f0a1c-cdc2-4671-bf75-5b3e5a2eeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "# sample_submission['Answer'] = preds\n",
    "# sample_submission.to_csv('../submission/gen_v4.03_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b90f0b-1997-4373-955e-0990fd4a2861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
