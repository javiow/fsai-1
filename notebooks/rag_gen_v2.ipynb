{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787c6027a9610f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas tqdm transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:16.029284Z",
     "start_time": "2025-08-04T09:40:16.025099Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9fbe2f1b329d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:18.581689Z",
     "start_time": "2025-08-04T09:40:18.542863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>금융산업의 이해와 관련하여 금융투자업의 구분에 해당하지 않는 것은?\\n1 소비자금융...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>위험 관리 계획 수립 시 고려해야 할 요소로 적절하지 않은 것은?\\n1 수행인력\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>재해 복구 계획 수립 시 고려해야 할 요소로 옳지 않은 것은?\\n1 복구 절차 수립...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>트로이 목마(Trojan) 기반 원격제어 악성코드(RAT)의 특징과 주요 탐지 지표...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>TEST_510</td>\n",
       "      <td>\"정보보호최고책임자\"의 임명에 관한 설명으로 옳지 않은 것은?\\n1 정보보호최고책임...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>TEST_511</td>\n",
       "      <td>IPv6 주소 체계의 주요 특징으로 옳지 않은 것은?\\n1 NAT 필요성 감소\\n2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>TEST_512</td>\n",
       "      <td>하이브리드 위협에 대한 설명으로 가장 적절한 것은?\\n1 사이버 공간에서만 발생하는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>TEST_513</td>\n",
       "      <td>전자금융거래법의 주요 목적 중 하나는 무엇인가?\\n1 전자금융거래의 비대면성 강화\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>TEST_514</td>\n",
       "      <td>전자서명인증업무 운영기준에 포함되지 않는 항목은 무엇인가?\\n1 전자서명인증서의 유...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                           Question\n",
       "0    TEST_000  금융산업의 이해와 관련하여 금융투자업의 구분에 해당하지 않는 것은?\\n1 소비자금융...\n",
       "1    TEST_001  위험 관리 계획 수립 시 고려해야 할 요소로 적절하지 않은 것은?\\n1 수행인력\\n...\n",
       "2    TEST_002  관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\\n...\n",
       "3    TEST_003  재해 복구 계획 수립 시 고려해야 할 요소로 옳지 않은 것은?\\n1 복구 절차 수립...\n",
       "4    TEST_004  트로이 목마(Trojan) 기반 원격제어 악성코드(RAT)의 특징과 주요 탐지 지표...\n",
       "..        ...                                                ...\n",
       "510  TEST_510  \"정보보호최고책임자\"의 임명에 관한 설명으로 옳지 않은 것은?\\n1 정보보호최고책임...\n",
       "511  TEST_511  IPv6 주소 체계의 주요 특징으로 옳지 않은 것은?\\n1 NAT 필요성 감소\\n2...\n",
       "512  TEST_512  하이브리드 위협에 대한 설명으로 가장 적절한 것은?\\n1 사이버 공간에서만 발생하는...\n",
       "513  TEST_513  전자금융거래법의 주요 목적 중 하나는 무엇인가?\\n1 전자금융거래의 비대면성 강화\\...\n",
       "514  TEST_514  전자서명인증업무 운영기준에 포함되지 않는 항목은 무엇인가?\\n1 전자서명인증서의 유...\n",
       "\n",
       "[515 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511d0f6574e94d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:31.325190Z",
     "start_time": "2025-08-04T09:40:31.318730Z"
    }
   },
   "outputs": [],
   "source": [
    "# 객관식 여부 판단 함수\n",
    "def is_multiple_choice(question_text):\n",
    "    \"\"\"\n",
    "    객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
    "    \"\"\"\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# 질문과 선택지 분리 함수\n",
    "def extract_question_and_choices(full_text):\n",
    "    \"\"\"\n",
    "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "    \"\"\"\n",
    "    lines = full_text.strip().split(\"\\n\")\n",
    "    q_lines = []\n",
    "    options = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "            options.append(line.strip())\n",
    "        else:\n",
    "            q_lines.append(line.strip())\n",
    "\n",
    "    question = \" \".join(q_lines)\n",
    "    return question, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8936cae9e5905b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:33.019426Z",
     "start_time": "2025-08-04T09:40:33.014808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 프롬프트 생성기\n",
    "def make_prompt_auto(text):\n",
    "    if is_multiple_choice(text):\n",
    "        question, options = extract_question_and_choices(text)\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                # \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"질문: {text}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c50b9e93981919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:50:33.144854Z",
     "start_time": "2025-08-04T09:40:35.988653Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# KULLM3 + FAISS(HNSW) 빠른 RAG (고정폭 lookbehind 수정 포함)\n",
    "\n",
    "import os, re, json, math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PDF_PATH = \"../data/개인정보 보호법.pdf\"\n",
    "LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 대체: intfloat/multilingual-e5-small\n",
    "TOP_K = 4\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 900\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 0) 경량 라우터 (정규식/키워드)\n",
    "LAW_KWS = (\"개인정보\", \"제\", \"조(\", \"시행령\", \"과징금\", \"처벌\", \"보안\", \"금융\", \"증권\", \"PPI\", \"CPI\", \"자본시장법\")\n",
    "def route_is_law(query: str) -> bool:\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in LAW_KWS)\n",
    "\n",
    "# ---------------- PDF 로드 & 정제 ----------------\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def clean_article_text(text: str) -> str:\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터', '', text)\n",
    "    text = re.sub(r'국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처|국가법령정보센터', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text() or \"\"\n",
    "        full_text += t + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def split_articles(text: str) -> List[Tuple[str, str, str]]:\n",
    "    pattern = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'\n",
    "    parts = re.split(pattern, text)\n",
    "    results = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m: \n",
    "            continue\n",
    "        art_id = m.group(1)\n",
    "        title = m.group(2)\n",
    "        results.append((art_id, title, clean_article_text(body)))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897a3e9b-e2c7-49b4-92b8-adad6e0a19e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 토큰 기준 청킹 ----------------\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    lookbehind 고정폭으로 분리:\n",
    "    - '...다.' 패턴 뒤 공백\n",
    "    - 일반 종결부호(. ? ! 。 ！ ？) 뒤 공백\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n",
    "\n",
    "def chunk_by_tokens(text: str, header: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(text)\n",
    "    if not sents:\n",
    "        sents = [text]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > max_tokens and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            if overlap > 0:\n",
    "                keep = cur[-1] if cur else \"\"\n",
    "                cur = [keep] if keep else []\n",
    "                cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "            else:\n",
    "                cur, cur_toks = [], token_len(prefix)\n",
    "        cur.append(s)\n",
    "        cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdeaf3b-c1a8-4e38-86d6-38975e2ad53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1385/4251994564.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 임베딩 & FAISS(HNSW) ----------------\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np, faiss\n",
    "from dataclasses import dataclass\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 128,\n",
    "                   \"convert_to_numpy\": True, \"convert_to_tensor\": False}\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    text: str\n",
    "    meta: dict\n",
    "\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 64) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(dim, m)\n",
    "    index.hnsw.efSearch = ef_search\n",
    "    index.add(vectors.astype(np.float32))\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c4719f-2c89-41e5-9df2-d3d19e18316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 데이터 준비 ----------------\n",
    "full_text = load_pdf_text(PDF_PATH)\n",
    "articles = split_articles(full_text)\n",
    "\n",
    "docs: List[Doc] = []\n",
    "for art_id, title, body in articles:\n",
    "    header = f\"개인정보 보호법 {art_id}({title})\"\n",
    "    for ch in chunk_by_tokens(body, header, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP):\n",
    "        docs.append(Doc(text=ch, meta={\n",
    "            \"article\": art_id, \"title\": title, \"tok_len\": token_len(ch)  # ← 캐시\n",
    "        }))\n",
    "\n",
    "corpus_texts = [d.text for d in docs]\n",
    "emb_matrix = np.array(embeddings.embed_documents(corpus_texts), dtype=np.float32)  # (N, D)\n",
    "\n",
    "# index = build_faiss_hnsw(emb_matrix, m=32, ef_search=64)\n",
    "index = build_faiss_hnsw(emb_matrix, m=32, ef_search=32)  # ← 64 -> 32 (보통 절반 가까이 빨라짐)\n",
    "\n",
    "\n",
    "def faiss_search(query: str, top_k: int = TOP_K) -> List[Doc]:\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    return [docs[int(i)] for i in I[0] if int(i) >= 0]\n",
    "\n",
    "# pack_context에서 캐시 활용 + 잘라붙일 때만 토크나이즈\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:  # 혹시 없는 경우만 계산\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24da7e19-e130-4040-aef4-795957cd3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685fa6ae610f401b93d4ce45d01f994d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x702e31fc8760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb230fa-1d19-4cf8-a817-972f797c312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 텍스트 생성 ----------------\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 128 if opt_cnt >= 2 else 256\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 한국 법령, 금융, 보안 도메인 Q/A를 담당하는 도우미입니다. \"\n",
    "    \"아는 사실만 간결하게 답하고, 모르면 '알 수 없습니다'라고 말하세요.\"\n",
    ")\n",
    "USER_TPL = (\n",
    "    \"다음 컨텍스트만 사용해 한국어로 정확하게 답하세요.\\n\"\n",
    "    \"===\\n{context}\\n===\\n질문: {query}\"\n",
    ")\n",
    "\n",
    "def faiss_search_with_scores(query: str, top_k: int = TOP_K):\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)              # L2 거리 (정규화 벡터)\n",
    "    cos = 1.0 - (D[0] / 2.0)                    # L2 -> cosine\n",
    "    out = []\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        if int(i) >= 0:\n",
    "            out.append((docs[int(i)], float(cos[idx])))\n",
    "    return out\n",
    "\n",
    "def generate_answer(query: str) -> str:\n",
    "    # (0) 라우팅: 법/금융/보안 질의가 아니면 검색 자체를 생략 → 즉시 베이스모델\n",
    "    if not route_is_law(query):\n",
    "        prompt = (\n",
    "            \"당신은 한국어로 간결하고 정확하게 답하는 도우미입니다. \"\n",
    "            \"사실에 근거해 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # (1) 벡터 검색 + 점수\n",
    "    scored = faiss_search_with_scores(query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # (2) 임계치(튜닝 포인트): 0.65 → 0.70로 올리면 컨텍스트 사용 빈도↓ → 평균 지연↓\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    # (3) 컨텍스트 조립 (캐시된 tok_len 사용)\n",
    "    ctx = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else \"\"\n",
    "\n",
    "    # (4) 프롬프트 구성 (불필요한 장식 최소화)\n",
    "    # 객관식 질문\n",
    "    if is_multiple_choice(query):\n",
    "        question, options = extract_question_and_choices(query)\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "    # 주관식 질문\n",
    "    else:\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "\n",
    "    # (5) 토크나이즈/생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0ba17ed873ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 함수\n",
    "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
    "    \"\"\"\n",
    "    - \"답변:\" 이후 텍스트만 추출\n",
    "    - 객관식 문제면: 정답 숫자만 추출 (실패 시 전체 텍스트 또는 기본값 반환)\n",
    "    - 주관식 문제면: 전체 텍스트 그대로 반환\n",
    "    - 공백 또는 빈 응답 방지: 최소 \"미응답\" 반환\n",
    "    \"\"\"\n",
    "    # \"답변:\" 기준으로 텍스트 분리\n",
    "    if \"답변:\" in generated_text:\n",
    "        text = generated_text.split(\"답변:\")[-1].strip()\n",
    "    else:\n",
    "        text = generated_text.strip()\n",
    "\n",
    "    # 공백 또는 빈 문자열일 경우 기본값 지정\n",
    "    if not text:\n",
    "        return \"미응답\"\n",
    "\n",
    "    # 객관식 여부 판단\n",
    "    is_mc = is_multiple_choice(original_question)\n",
    "\n",
    "    if is_mc:\n",
    "        # 숫자만 추출\n",
    "        match = re.match(r\"\\D*([1-9][0-9]?)\", text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            # 숫자 추출 실패 시 \"0\" 반환\n",
    "            return \"0\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8bd6175f1111c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/515 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Question ###########################\n",
      "금융산업의 이해와 관련하여 금융투자업의 구분에 해당하지 않는 것은?\n",
      "1 소비자금융업\n",
      "2 투자자문업\n",
      "3 투자매매업\n",
      "4 투자중개업\n",
      "5 보험중개업\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 1/515 [00:02<19:45,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "5\n",
      "#################### Question ###########################\n",
      "위험 관리 계획 수립 시 고려해야 할 요소로 적절하지 않은 것은?\n",
      "1 수행인력\n",
      "2 위험 수용\n",
      "3 위험 대응 전략 선정\n",
      "4 대상\n",
      "5 기간\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Inference:   0%|          | 2/515 [00:04<18:37,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "4\n",
      "#################### Question ###########################\n",
      "관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\n",
      "1 정보보호 및 개인정보보호 정책의 제·개정\n",
      "2 경영진의 참여\n",
      "3 최고책임자의 지정\n",
      "4 자원 할당\n",
      "5 내부 감사 절차의 수립\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   1%|          | 3/515 [00:20<1:11:06,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "1\n",
      "#################### Question ###########################\n",
      "재해 복구 계획 수립 시 고려해야 할 요소로 옳지 않은 것은?\n",
      "1 복구 절차 수립\n",
      "2 비상연락체계 구축\n",
      "3 개인정보 파기 절차\n",
      "4 복구 목표시간 정의\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   1%|          | 4/515 [00:20<44:58,  5.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "3\n",
      "#################### Question ###########################\n",
      "트로이 목마(Trojan) 기반 원격제어 악성코드(RAT)의 특징과 주요 탐지 지표를 설명하세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   1%|          | 5/515 [01:15<3:15:22, 22.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "트로이 목마 기반 원격제어 악성코드(RAT)는 사용자의 지식 없이 시스템을 제어하고, 데이터를 훔치거나 시스템을 손상시킬 수 있는 능력을 가지고 있습니다. 이러한 악성코드는 사용자의 클라이언트 컴퓨터에 침투하여 백신 시스템을 비활성화하고, 사용자의 동의 없이 데이터를 송수신할 수 있습니다. 주요 탐지 지표로는 이러한 악성코드가 시스템 리소스를 과도하게 사용하거나, 이상한 네트워크 활동을 보이는 것이 있습니다.\n",
      "#################### Question ###########################\n",
      "한국은행이 금융통화위원회의 요청에 따라 금융회사 및 전자금융업자에게 자료제출을 요구할 수 있는 경우는?\n",
      "1 전자금융거래의 보안 강화를 위해\n",
      "2 전자금융거래의 통계조사를 위해\n",
      "3 금융회사의 경영 실적 분석을 위해\n",
      "4 통화신용정책의 수행 및 지급결제제도의 원활한 운영을 위해\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   1%|          | 6/515 [01:23<2:32:53, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "4\n",
      "#################### Question ###########################\n",
      "개인정보보호법 제22조의2에 따라 만 14세 미만 아동의 개인정보를 처리하기 위해 필요한 절차로 옳은 것은?\n",
      "1 아동의 학교의 동의를 받아야 한다.\n",
      "2 법정대리인의 동의를 받아야 한다.\n",
      "3 아동 본인의 동의만 받으면 된다.\n",
      "4 아동의 친구의 동의를 받아야 한다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   1%|▏         | 7/515 [01:35<2:16:54, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "2\n",
      "#################### Question ###########################\n",
      "전자금융거래법에 따라 이용자가 금융 분쟁조정을 신청할 수 있는 기관을 기술하세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   2%|▏         | 8/515 [02:09<3:04:30, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Answer ###########################\n",
      "전자금융거래법에 따라 이용자가 금융 분쟁조정을 신청할 수 있는 기관은 금융분쟁조정기관입니다.\n",
      "#################### Question ###########################\n",
      "금융권에서 SBOM을 활용하는 이유로 가장 적절한 것은?\n",
      "1 금융 시스템의 접근 제어 정책을 효율적으로 구현하기 위해\n",
      "2 금융 거래의 투명성을 높이기 위해\n",
      "3 고객의 개인정보 보호를 강화하기 위해\n",
      "4 금융 상품의 다양성을 확보하기 위해\n",
      "5 S/W 공급망 공격을 예방하기 위해\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
    "    print(\"#################### Question ###########################\")\n",
    "    print(q)\n",
    "    ans = generate_answer(q)\n",
    "    pred_answer = extract_answer_only(ans, original_question=q)\n",
    "    print(\"#################### Answer ###########################\")\n",
    "    print(pred_answer)\n",
    "    preds.append(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634101eaf0d5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['Answer'] = preds\n",
    "sample_submission.to_csv('../submission/gen_v2_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
