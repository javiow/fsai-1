{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2787c6027a9610f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas tqdm transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:16.029284Z",
     "start_time": "2025-08-04T09:40:16.025099Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fbe2f1b329d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:18.581689Z",
     "start_time": "2025-08-04T09:40:18.542863Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d0f6574e94d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:31.325190Z",
     "start_time": "2025-08-04T09:40:31.318730Z"
    }
   },
   "outputs": [],
   "source": [
    "# 객관식 여부 판단 함수\n",
    "def is_multiple_choice(question_text):\n",
    "    \"\"\"\n",
    "    객관식 여부를 판단: 2개 이상의 숫자 선택지가 줄 단위로 존재할 경우 객관식으로 간주\n",
    "    \"\"\"\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "\n",
    "# 질문과 선택지 분리 함수\n",
    "def extract_question_and_choices(full_text):\n",
    "    \"\"\"\n",
    "    전체 질문 문자열에서 질문 본문과 선택지 리스트를 분리\n",
    "    \"\"\"\n",
    "    lines = full_text.strip().split(\"\\n\")\n",
    "    q_lines = []\n",
    "    options = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "            options.append(line.strip())\n",
    "        else:\n",
    "            q_lines.append(line.strip())\n",
    "\n",
    "    question = \" \".join(q_lines)\n",
    "    return question, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8936cae9e5905b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:40:33.019426Z",
     "start_time": "2025-08-04T09:40:33.014808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 프롬프트 생성기\n",
    "def make_prompt_auto(text):\n",
    "    if is_multiple_choice(text):\n",
    "        question, options = extract_question_and_choices(text)\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    else:\n",
    "        prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                # \"아래 주관식 질문에 대해 정확하고 간략한 설명을 작성하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"질문: {text}\\n\\n\"\n",
    "                \"답변:\"\n",
    "                )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c50b9e93981919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T09:50:33.144854Z",
     "start_time": "2025-08-04T09:40:35.988653Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 멀티법령 RAG 전처리/인덱싱 (PIPA, 신용정보법, 전자서명법, 정보통신망법)\n",
    "\n",
    "import os, re, json, math\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "\n",
    "# ===== 사용자 환경 상수 =====\n",
    "LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 경량/호환성 위주\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 600\n",
    "TOP_K = 4\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ===== 법령 설정: 파일 경로 + 제거/정규식 패턴 =====\n",
    "LAW_CONFIG = {\n",
    "    # 개인정보 보호법\n",
    "    \"pipa\": {\n",
    "        \"law_name\": \"개인정보 보호법\",\n",
    "        \"pdf_path\": \"../data/개인정보 보호법(법률)(제19234호)(20250313).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터',\n",
    "            r'국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>',         # <개정 …>, <신설 …>\n",
    "            r'\\[[^\\]]+\\]',      # [본조신설 …]\n",
    "        ],\n",
    "    },\n",
    "    # 신용정보의 이용 및 보호에 관한 법률\n",
    "    \"ciupa\": {\n",
    "        \"law_name\": \"신용정보법\",\n",
    "        \"pdf_path\": \"../data/신용정보의 이용 및 보호에 관한 법률(법률)(제20304호)(20240814).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*신용정보.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자서명법\n",
    "    \"es_act\": {\n",
    "        \"law_name\": \"전자서명법\",\n",
    "        \"pdf_path\": \"../data/전자서명법(법률)(제18479호)(20221020).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자서명법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 정보통신망 이용촉진 및 정보보호 등에 관한 법률\n",
    "    \"icn_act\": {\n",
    "        \"law_name\": \"정보통신망법\",\n",
    "        \"pdf_path\": \"../data/정보통신망 이용촉진 및 정보보호 등에 관한 법률(법률)(제20678호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*정보통신망.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a3e9b-e2c7-49b4-92b8-adad6e0a19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 토크나이저: 토큰 길이 계산/청킹 =====\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    # 고정폭 lookbehind: '다.' 또는 일반 종결부호\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdeaf3b-c1a8-4e38-86d6-38975e2ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 공통 정제 =====\n",
    "def normalize_common(text: str) -> str:\n",
    "    # 한자 제거\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    # circled numbers → (n)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    # 공백/빈 괄호 정리\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_by_config(text: str, drop_patterns: List[str]) -> str:\n",
    "    for pat in drop_patterns:\n",
    "        text = re.sub(pat, '', text)\n",
    "    return normalize_common(text)\n",
    "\n",
    "# ===== 조문 단위 분리 =====\n",
    "ARTICLE_HEADER_PATTERN = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'  # 제X조(제목) / 제X조의Y(제목)\n",
    "\n",
    "def split_articles(raw_text: str) -> List[Tuple[str, str, str]]:\n",
    "    parts = re.split(ARTICLE_HEADER_PATTERN, raw_text)\n",
    "    out = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m:\n",
    "            continue\n",
    "        article_id = m.group(1)          # 제xx조 / 제xx조의y\n",
    "        article_title = m.group(2)       # (제목)\n",
    "        out.append((article_id, article_title, body))\n",
    "    return out\n",
    "\n",
    "def chunk_article(article_body: str, header: str) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(article_body) or [article_body]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > CHUNK_TOKENS and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            # overlap: 마지막 문장 유지\n",
    "            keep = cur[-1] if CHUNK_OVERLAP > 0 and cur else \"\"\n",
    "            cur = [keep] if keep else []\n",
    "            cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "        cur.append(s); cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4719f-2c89-41e5-9df2-d3d19e18316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 데이터 클래스 =====\n",
    "@dataclass\n",
    "class LawDoc:\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "# ===== 임베딩 =====\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 128,\n",
    "        \"convert_to_numpy\": True,\n",
    "        \"convert_to_tensor\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# ===== 인덱스 (FAISS HNSW) =====\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 32) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, m)\n",
    "    idx.hnsw.efSearch = ef_search\n",
    "    idx.add(vectors.astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "# ===== 파이프라인: 1) 로드 → 2) 정제 → 3) 조문분리 → 4) 청킹 → 5) 임베딩/인덱스\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        text += t + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def preprocess_law(law_id: str, cfg: Dict) -> List[LawDoc]:\n",
    "    raw = load_pdf_text(cfg[\"pdf_path\"])\n",
    "    cleaned = clean_text_by_config(raw, cfg[\"drop_patterns\"])\n",
    "    articles = split_articles(cleaned)\n",
    "\n",
    "    docs: List[LawDoc] = []\n",
    "    # 시행일 파싱(있으면): [시행 YYYY. M. D.]가 머리말에 있는 경우가 많으나 PDF마다 다름 → 필요 시 별도 파서\n",
    "    effective_date = None  # 필요 시 추출 로직 추가\n",
    "\n",
    "    for article_id, title, body in articles:\n",
    "        header = f'{cfg[\"law_name\"]} {article_id}({title})'\n",
    "        chunks = chunk_article(body, header)\n",
    "        for ch in chunks:\n",
    "            meta = {\n",
    "                \"law_id\": law_id,\n",
    "                \"law_name\": cfg[\"law_name\"],\n",
    "                \"article_id\": article_id,\n",
    "                \"article_title\": title,\n",
    "                \"effective_date\": effective_date,   # None 가능\n",
    "                \"tok_len\": token_len(ch),\n",
    "                \"source_uri\": cfg.get(\"pdf_path\"),\n",
    "                \"version\": None\n",
    "            }\n",
    "            docs.append(LawDoc(text=ch, meta=meta))\n",
    "    return docs\n",
    "\n",
    "def build_indices(all_docs: List[LawDoc]):\n",
    "    # (a) 법령별 인덱스\n",
    "    per_law_docs: Dict[str, List[LawDoc]] = {}\n",
    "    for d in all_docs:\n",
    "        per_law_docs.setdefault(d.meta[\"law_id\"], []).append(d)\n",
    "\n",
    "    indices = {}\n",
    "    for law_id, docs in per_law_docs.items():\n",
    "        mat = np.array(embeddings.embed_documents([d.text for d in docs]), dtype=np.float32)\n",
    "        indices[f\"faiss_hnsw_{law_id}\"] = {\n",
    "            \"index\": build_faiss_hnsw(mat, m=32, ef_search=32),\n",
    "            \"docs\": docs\n",
    "        }\n",
    "\n",
    "    # (b) 글로벌 인덱스\n",
    "    mat_all = np.array(embeddings.embed_documents([d.text for d in all_docs]), dtype=np.float32)\n",
    "    indices[\"faiss_hnsw_all\"] = {\n",
    "        \"index\": build_faiss_hnsw(mat_all, m=32, ef_search=32),\n",
    "        \"docs\": all_docs\n",
    "    }\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da7e19-e130-4040-aef4-795957cd3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs: List[LawDoc] = []\n",
    "for law_id, cfg in LAW_CONFIG.items():\n",
    "    if not os.path.exists(cfg[\"pdf_path\"]):\n",
    "        print(f\"[WARN] PDF not found: {cfg['pdf_path']}\")\n",
    "        continue\n",
    "    docs = preprocess_law(law_id, cfg)\n",
    "    all_docs.extend(docs)\n",
    "    print(f\"[OK] {cfg['law_name']} → chunks: {len(docs)}\")\n",
    "\n",
    "indices = build_indices(all_docs)\n",
    "print(\"[OK] built indices:\", list(indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c173872-00c8-4149-8327-1836b6a312b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 라우팅 규칙 =====\n",
    "ARTICLE_PTRN = re.compile(r\"제\\d+조(?:의\\d+)?\")  # 조문 표기\n",
    "LAW_HINTS = {\n",
    "    \"pipa\": (\"개인정보\", \"개인 정보\", \"개인정보보호법\"),\n",
    "    \"ciupa\": (\"신용정보\",  \"신용정보법\"),\n",
    "    \"es_act\": (\"전자서명\", \"전자서명법\"),\n",
    "    \"icn_act\": (\"정보통신망\", \"통신망\", \"정보통신망법\"),\n",
    "}\n",
    "\n",
    "def detect_law_id(query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    for law_id, kws in LAW_HINTS.items():\n",
    "        if any(kw.lower() in q for kw in kws):\n",
    "            return law_id\n",
    "    return None\n",
    "\n",
    "def route_is_domain(query: str) -> bool:\n",
    "    # 법/금융/보안 도메인 간단 라우터 (검색 여부 판단용)\n",
    "    domain_kws = (\"법\", \"조(\", \"과징금\", \"처벌\", \"보안\", \"침해\", \"금융\", \"증권\", \"자본시장\", \"개인정보\", \"신용정보\", \"전자서명\", \"정보통신망\")\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in domain_kws) or bool(ARTICLE_PTRN.search(query))\n",
    "\n",
    "def choose_index(indices: dict, query: str):\n",
    "    \"\"\"\n",
    "    1) 질의에서 법령 단서 -> 해당 법 인덱스 우선\n",
    "    2) 조문 패턴만 있거나 단서가 불분명 -> 글로벌 인덱스\n",
    "    3) 아무 단서도 없으면 None (베이스모델 경로)\n",
    "    \"\"\"\n",
    "    law_id = detect_law_id(query)\n",
    "    if law_id:\n",
    "        key = f\"faiss_hnsw_{law_id}\"\n",
    "        if key in indices:\n",
    "            return indices[key]  # {\"index\": ..., \"docs\": ...}\n",
    "    # 법령 단서 없지만 도메인성/조문 표기는 있는 경우 글로벌\n",
    "    if route_is_domain(query) and \"faiss_hnsw_all\" in indices:\n",
    "        return indices[\"faiss_hnsw_all\"]\n",
    "    return None  # 베이스모델 직행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849912de-b930-4c92-9909-2e759fbf42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 검색 with 점수 (해당 인덱스에서) =====\n",
    "def faiss_search_with_scores_from_index(index_entry: dict, query: str, top_k: int = TOP_K):\n",
    "    # embeddings는 상위 스코프에서 로드되었다고 가정\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index_entry[\"index\"].search(qv, top_k)     # L2 거리 (정규화 벡터 가정)\n",
    "    cos = 1.0 - (D[0] / 2.0)                          # L2 -> cosine\n",
    "    out = []\n",
    "    docs = index_entry[\"docs\"]\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        ii = int(i)\n",
    "        if ii >= 0:\n",
    "            out.append((docs[ii], float(cos[idx])))\n",
    "    return out  # [(LawDoc, cosine), ...]\n",
    "\n",
    "# ===== 컨텍스트 패킹 (질문 주신 코드 재사용 + tok_len 캐시) =====\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f810e-a409-42fb-879c-739125042131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 프롬프트 =====\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 한국 법령, 금융, 보안 도메인 Q/A를 담당하는 도우미입니다. \"\n",
    "    \"아는 사실만 간결하게 답하고, 모르면 '알 수 없습니다'라고 말하세요.\"\n",
    ")\n",
    "\n",
    "def build_prompt(query: str, use_context: bool) -> Tuple[str, int]:\n",
    "    \"\"\"컨텍스트 유무에 따라 간단 프롬프트와 max_len을 반환\"\"\"\n",
    "    # 객관식 질문\n",
    "    if is_multiple_choice(query):\n",
    "        question, options = extract_question_and_choices(query)\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "            return prompt, max_len\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"선택지:\\n\"\n",
    "                f\"{chr(10).join(options)}\\n\\n\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "            return prompt, max_len\n",
    "    # 주관식 질문\n",
    "    else:\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "            return prompt, max_len\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"당신은 금융보안 전문가입니다.\\n\"\n",
    "                \"아래 질문에 대해 정답의 핵심 키워드와 의미를 모두 포함하여 3문장 이내로 간결하게 답변하세요. 군더더기 없이 요점만 명확하게 작성하세요.\\n\\n\"\n",
    "                f\"질문: {query}\"\n",
    "                \"답변:\"\n",
    "            )\n",
    "            max_new = dynamic_max_new_tokens(query)\n",
    "            max_len = 2048\n",
    "            return prompt, max_len\n",
    "\n",
    "\n",
    "# ===== 생성 토큰 상한 =====\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 96 if opt_cnt >= 2 else 192\n",
    "\n",
    "# ===== 메인: 다중 인덱스 기반 텍스트 생성 =====\n",
    "def generate_answer_with_indices(query: str, indices: dict) -> str:\n",
    "    \"\"\"\n",
    "    indices: build_indices() 반환 구조\n",
    "    - 라우팅 → 해당 인덱스에서 top-k 검색(점수 포함)\n",
    "    - 최고 유사도 임계치 미만이면 컨텍스트 없이 베이스모델 생성(Adaptive RAG)\n",
    "    \"\"\"\n",
    "    # 0) 우선, 일반상식/비도메인은 곧장 베이스모델\n",
    "    if not route_is_domain(query):\n",
    "        prompt, max_len = build_prompt(query, use_context=False)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 1) 인덱스 선택\n",
    "    idx_entry = choose_index(indices, query)\n",
    "\n",
    "    # 2) 인덱스가 없으면 베이스모델\n",
    "    if idx_entry is None:\n",
    "        prompt, max_len = build_prompt(query, use_context=False)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 3) 선택 인덱스에서 검색 + 점수\n",
    "    scored = faiss_search_with_scores_from_index(idx_entry, query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # 4) 임계치: 충분히 유사할 때만 컨텍스트 사용 (속도 최적화)\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    context = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else None\n",
    "    prompt, max_len = build_prompt(query, use_context)\n",
    "\n",
    "    # 5) LLM 생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=dynamic_max_new_tokens(query),\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9f6a5-b6eb-4187-9b78-ed6d262a9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ba17ed873ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리 함수\n",
    "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
    "    \"\"\"\n",
    "    - \"답변:\" 이후 텍스트만 추출\n",
    "    - 객관식 문제면: 정답 숫자만 추출 (실패 시 전체 텍스트 또는 기본값 반환)\n",
    "    - 주관식 문제면: 전체 텍스트 그대로 반환\n",
    "    - 공백 또는 빈 응답 방지: 최소 \"미응답\" 반환\n",
    "    \"\"\"\n",
    "    # \"답변:\" 기준으로 텍스트 분리\n",
    "    if \"답변:\" in generated_text:\n",
    "        text = generated_text.split(\"답변:\")[-1].strip()\n",
    "    else:\n",
    "        text = generated_text.strip()\n",
    "\n",
    "    # 공백 또는 빈 문자열일 경우 기본값 지정\n",
    "    if not text:\n",
    "        return \"미응답\"\n",
    "\n",
    "    # 객관식 여부 판단\n",
    "    is_mc = is_multiple_choice(original_question)\n",
    "\n",
    "    if is_mc:\n",
    "        # 숫자만 추출\n",
    "        match = re.match(r\"\\D*([1-9][0-9]?)\", text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            # 숫자 추출 실패 시 \"0\" 반환\n",
    "            return \"0\"\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8bd6175f1111c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
    "    print(\"#################### Question ###########################\")\n",
    "    print(q)\n",
    "    ans = generate_answer_with_indices(q, indices)\n",
    "    pred_answer = extract_answer_only(ans, original_question=q)\n",
    "    print(\"#################### Answer ###########################\")\n",
    "    print(pred_answer)\n",
    "    preds.append(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634101eaf0d5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../submission/sample_submission.csv')\n",
    "sample_submission['Answer'] = preds\n",
    "sample_submission.to_csv('../submission/gen_v3_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
