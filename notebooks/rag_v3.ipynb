{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bc6bb1-69e6-48ff-958d-f2f4f5a38ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 멀티법령 RAG 전처리/인덱싱 (PIPA, 신용정보법, 전자서명법, 정보통신망법)\n",
    "\n",
    "import os, re, json, math\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "\n",
    "# ===== 사용자 환경 상수 =====\n",
    "LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 경량/호환성 위주\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 600\n",
    "TOP_K = 4\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ===== 법령 설정: 파일 경로 + 제거/정규식 패턴 =====\n",
    "LAW_CONFIG = {\n",
    "    # 개인정보 보호법\n",
    "    \"pipa\": {\n",
    "        \"law_name\": \"개인정보 보호법\",\n",
    "        \"pdf_path\": \"../data/개인정보 보호법(법률)(제19234호)(20250313).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터',\n",
    "            r'국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>',         # <개정 …>, <신설 …>\n",
    "            r'\\[[^\\]]+\\]',      # [본조신설 …]\n",
    "        ],\n",
    "    },\n",
    "    # 신용정보의 이용 및 보호에 관한 법률\n",
    "    \"ciupa\": {\n",
    "        \"law_name\": \"신용정보법\",\n",
    "        \"pdf_path\": \"../data/신용정보의 이용 및 보호에 관한 법률(법률)(제20304호)(20240814).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*신용정보.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자서명법\n",
    "    \"es_act\": {\n",
    "        \"law_name\": \"전자서명법\",\n",
    "        \"pdf_path\": \"../data/전자서명법(법률)(제18479호)(20221020).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자서명법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 정보통신망 이용촉진 및 정보보호 등에 관한 법률\n",
    "    \"icn_act\": {\n",
    "        \"law_name\": \"정보통신망법\",\n",
    "        \"pdf_path\": \"../data/정보통신망 이용촉진 및 정보보호 등에 관한 법률(법률)(제20678호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*정보통신망.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914873a0-d4d8-44f9-9fd8-0c25da8f84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== 토크나이저: 토큰 길이 계산/청킹 =====\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    # 고정폭 lookbehind: '다.' 또는 일반 종결부호\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82393c99-d378-449a-b4a8-edc5639378b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 공통 정제 =====\n",
    "def normalize_common(text: str) -> str:\n",
    "    # 한자 제거\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    # circled numbers → (n)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    # 공백/빈 괄호 정리\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_by_config(text: str, drop_patterns: List[str]) -> str:\n",
    "    for pat in drop_patterns:\n",
    "        text = re.sub(pat, '', text)\n",
    "    return normalize_common(text)\n",
    "\n",
    "# ===== 조문 단위 분리 =====\n",
    "ARTICLE_HEADER_PATTERN = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'  # 제X조(제목) / 제X조의Y(제목)\n",
    "\n",
    "def split_articles(raw_text: str) -> List[Tuple[str, str, str]]:\n",
    "    parts = re.split(ARTICLE_HEADER_PATTERN, raw_text)\n",
    "    out = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m:\n",
    "            continue\n",
    "        article_id = m.group(1)          # 제xx조 / 제xx조의y\n",
    "        article_title = m.group(2)       # (제목)\n",
    "        out.append((article_id, article_title, body))\n",
    "    return out\n",
    "\n",
    "def chunk_article(article_body: str, header: str) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(article_body) or [article_body]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > CHUNK_TOKENS and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            # overlap: 마지막 문장 유지\n",
    "            keep = cur[-1] if CHUNK_OVERLAP > 0 and cur else \"\"\n",
    "            cur = [keep] if keep else []\n",
    "            cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "        cur.append(s); cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04adcca9-7f2f-41a4-9fb0-ebcb87dc3742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3220/3198458753.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== 데이터 클래스 =====\n",
    "@dataclass\n",
    "class LawDoc:\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "# ===== 임베딩 =====\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 128,\n",
    "        \"convert_to_numpy\": True,\n",
    "        \"convert_to_tensor\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# ===== 인덱스 (FAISS HNSW) =====\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 32) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, m)\n",
    "    idx.hnsw.efSearch = ef_search\n",
    "    idx.add(vectors.astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "# ===== 파이프라인: 1) 로드 → 2) 정제 → 3) 조문분리 → 4) 청킹 → 5) 임베딩/인덱스\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        text += t + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def preprocess_law(law_id: str, cfg: Dict) -> List[LawDoc]:\n",
    "    raw = load_pdf_text(cfg[\"pdf_path\"])\n",
    "    cleaned = clean_text_by_config(raw, cfg[\"drop_patterns\"])\n",
    "    articles = split_articles(cleaned)\n",
    "\n",
    "    docs: List[LawDoc] = []\n",
    "    # 시행일 파싱(있으면): [시행 YYYY. M. D.]가 머리말에 있는 경우가 많으나 PDF마다 다름 → 필요 시 별도 파서\n",
    "    effective_date = None  # 필요 시 추출 로직 추가\n",
    "\n",
    "    for article_id, title, body in articles:\n",
    "        header = f'{cfg[\"law_name\"]} {article_id}({title})'\n",
    "        chunks = chunk_article(body, header)\n",
    "        for ch in chunks:\n",
    "            meta = {\n",
    "                \"law_id\": law_id,\n",
    "                \"law_name\": cfg[\"law_name\"],\n",
    "                \"article_id\": article_id,\n",
    "                \"article_title\": title,\n",
    "                \"effective_date\": effective_date,   # None 가능\n",
    "                \"tok_len\": token_len(ch),\n",
    "                \"source_uri\": cfg.get(\"pdf_path\"),\n",
    "                \"version\": None\n",
    "            }\n",
    "            docs.append(LawDoc(text=ch, meta=meta))\n",
    "    return docs\n",
    "\n",
    "def build_indices(all_docs: List[LawDoc]):\n",
    "    # (a) 법령별 인덱스\n",
    "    per_law_docs: Dict[str, List[LawDoc]] = {}\n",
    "    for d in all_docs:\n",
    "        per_law_docs.setdefault(d.meta[\"law_id\"], []).append(d)\n",
    "\n",
    "    indices = {}\n",
    "    for law_id, docs in per_law_docs.items():\n",
    "        mat = np.array(embeddings.embed_documents([d.text for d in docs]), dtype=np.float32)\n",
    "        indices[f\"faiss_hnsw_{law_id}\"] = {\n",
    "            \"index\": build_faiss_hnsw(mat, m=32, ef_search=32),\n",
    "            \"docs\": docs\n",
    "        }\n",
    "\n",
    "    # (b) 글로벌 인덱스\n",
    "    mat_all = np.array(embeddings.embed_documents([d.text for d in all_docs]), dtype=np.float32)\n",
    "    indices[\"faiss_hnsw_all\"] = {\n",
    "        \"index\": build_faiss_hnsw(mat_all, m=32, ef_search=32),\n",
    "        \"docs\": all_docs\n",
    "    }\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b052bc-0b49-459b-a779-ef1513035f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 개인정보 보호법 → chunks: 217\n",
      "[OK] 신용정보법 → chunks: 181\n",
      "[OK] 전자서명법 → chunks: 29\n",
      "[OK] 정보통신망법 → chunks: 151\n",
      "[OK] built indices: ['faiss_hnsw_pipa', 'faiss_hnsw_ciupa', 'faiss_hnsw_es_act', 'faiss_hnsw_icn_act', 'faiss_hnsw_all']\n"
     ]
    }
   ],
   "source": [
    "all_docs: List[LawDoc] = []\n",
    "for law_id, cfg in LAW_CONFIG.items():\n",
    "    if not os.path.exists(cfg[\"pdf_path\"]):\n",
    "        print(f\"[WARN] PDF not found: {cfg['pdf_path']}\")\n",
    "        continue\n",
    "    docs = preprocess_law(law_id, cfg)\n",
    "    all_docs.extend(docs)\n",
    "    print(f\"[OK] {cfg['law_name']} → chunks: {len(docs)}\")\n",
    "\n",
    "indices = build_indices(all_docs)\n",
    "print(\"[OK] built indices:\", list(indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2288fa93-7918-489b-9678-f821b3942121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 라우팅 규칙 =====\n",
    "ARTICLE_PTRN = re.compile(r\"제\\d+조(?:의\\d+)?\")  # 조문 표기\n",
    "LAW_HINTS = {\n",
    "    \"pipa\": (\"개인정보\", \"개인 정보\", \"개인정보보호법\"),\n",
    "    \"ciupa\": (\"신용정보\",  \"신용정보법\"),\n",
    "    \"es_act\": (\"전자서명\", \"전자서명법\"),\n",
    "    \"icn_act\": (\"정보통신망\", \"통신망\", \"정보통신망법\"),\n",
    "}\n",
    "\n",
    "def detect_law_id(query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    for law_id, kws in LAW_HINTS.items():\n",
    "        if any(kw.lower() in q for kw in kws):\n",
    "            return law_id\n",
    "    return None\n",
    "\n",
    "def route_is_domain(query: str) -> bool:\n",
    "    # 법/금융/보안 도메인 간단 라우터 (검색 여부 판단용)\n",
    "    domain_kws = (\"법\", \"조(\", \"과징금\", \"처벌\", \"보안\", \"침해\", \"금융\", \"증권\", \"자본시장\", \"개인정보\", \"신용정보\", \"전자서명\", \"정보통신망\")\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in domain_kws) or bool(ARTICLE_PTRN.search(query))\n",
    "\n",
    "def choose_index(indices: dict, query: str):\n",
    "    \"\"\"\n",
    "    1) 질의에서 법령 단서 -> 해당 법 인덱스 우선\n",
    "    2) 조문 패턴만 있거나 단서가 불분명 -> 글로벌 인덱스\n",
    "    3) 아무 단서도 없으면 None (베이스모델 경로)\n",
    "    \"\"\"\n",
    "    law_id = detect_law_id(query)\n",
    "    if law_id:\n",
    "        key = f\"faiss_hnsw_{law_id}\"\n",
    "        if key in indices:\n",
    "            return indices[key]  # {\"index\": ..., \"docs\": ...}\n",
    "    # 법령 단서 없지만 도메인성/조문 표기는 있는 경우 글로벌\n",
    "    if route_is_domain(query) and \"faiss_hnsw_all\" in indices:\n",
    "        return indices[\"faiss_hnsw_all\"]\n",
    "    return None  # 베이스모델 직행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ff0176-2a7e-440a-b789-f285a51dbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 검색 with 점수 (해당 인덱스에서) =====\n",
    "def faiss_search_with_scores_from_index(index_entry: dict, query: str, top_k: int = TOP_K):\n",
    "    # embeddings는 상위 스코프에서 로드되었다고 가정\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index_entry[\"index\"].search(qv, top_k)     # L2 거리 (정규화 벡터 가정)\n",
    "    cos = 1.0 - (D[0] / 2.0)                          # L2 -> cosine\n",
    "    out = []\n",
    "    docs = index_entry[\"docs\"]\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        ii = int(i)\n",
    "        if ii >= 0:\n",
    "            out.append((docs[ii], float(cos[idx])))\n",
    "    return out  # [(LawDoc, cosine), ...]\n",
    "\n",
    "# ===== 컨텍스트 패킹 (질문 주신 코드 재사용 + tok_len 캐시) =====\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8328f62e-c4d8-471e-b01b-719c13e6d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 프롬프트 =====\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 한국 법령, 금융, 보안 도메인 Q/A를 담당하는 도우미입니다. \"\n",
    "    \"아는 사실만 간결하게 답하고, 모르면 '알 수 없습니다'라고 말하세요.\"\n",
    ")\n",
    "\n",
    "def build_prompt(query: str, context: Optional[str]) -> Tuple[str, int]:\n",
    "    \"\"\"컨텍스트 유무에 따라 간단 프롬프트와 max_len을 반환\"\"\"\n",
    "    if context and context.strip():\n",
    "        prompt = (\n",
    "            \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "            f\"=== 컨텍스트 ===\\n{context}\\n=== 끝 ===\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        return f\"{SYSTEM_PROMPT}\\n\\n{prompt}\", 3072\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"당신은 한국어로 간결하고 정확하게 답하는 도우미입니다. \"\n",
    "            \"사실에 근거해 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        return prompt, 2048\n",
    "\n",
    "# ===== 생성 토큰 상한 =====\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 96 if opt_cnt >= 2 else 192\n",
    "\n",
    "# ===== 메인: 다중 인덱스 기반 텍스트 생성 =====\n",
    "def generate_answer_with_indices(query: str, indices: dict) -> str:\n",
    "    \"\"\"\n",
    "    indices: build_indices() 반환 구조\n",
    "    - 라우팅 → 해당 인덱스에서 top-k 검색(점수 포함)\n",
    "    - 최고 유사도 임계치 미만이면 컨텍스트 없이 베이스모델 생성(Adaptive RAG)\n",
    "    \"\"\"\n",
    "    # 0) 우선, 일반상식/비도메인은 곧장 베이스모델\n",
    "    if not route_is_domain(query):\n",
    "        prompt, max_len = build_prompt(query, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 1) 인덱스 선택\n",
    "    idx_entry = choose_index(indices, query)\n",
    "\n",
    "    # 2) 인덱스가 없으면 베이스모델\n",
    "    if idx_entry is None:\n",
    "        prompt, max_len = build_prompt(query, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 3) 선택 인덱스에서 검색 + 점수\n",
    "    scored = faiss_search_with_scores_from_index(idx_entry, query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # 4) 임계치: 충분히 유사할 때만 컨텍스트 사용 (속도 최적화)\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    context = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else None\n",
    "    prompt, max_len = build_prompt(query, context)\n",
    "\n",
    "    # 5) LLM 생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=dynamic_max_new_tokens(query),\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4e5fed-0733-47c3-bb2c-a4383b467e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17d40a43d774da1af7fce0f52cd369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x754948a82c20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a33789-82f3-44d7-a509-a6a1e8d2eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답: 1 60일 이내\n"
     ]
    }
   ],
   "source": [
    "# q = \"\"\"개인정보보호법 제22조의2에 따라 만 14세 미만 아동의 개인정보를 처리하기 위해 필요한 절차로 옳은 것은?\n",
    "# 1 아동의 학교의 동의를 받아야 한다.\n",
    "# 2 법정대리인의 동의를 받아야 한다.\n",
    "# 3 아동 본인의 동의만 받으면 된다.\n",
    "# 4 아동의 친구의 동의를 받아야 한다.\"\"\"\n",
    "\n",
    "# q = \"\"\"고려대학교에 대해서 설명해줘.\"\"\"\n",
    "\n",
    "# q = \"\"\"정보통신망법 제44조의2에 따르면, 정보통신망을 통해 공개된 정보로 인해 사생활 침해나 명예훼손이 발생한 경우, 침해를 받은 자가 정보통신서비스 제공자에게 요청할 수 있는 조치는 무엇인가?\n",
    "# 1 정보의 공개 및 공유\n",
    "# 2 정보의 수정 및 재배포\n",
    "# 3 정보의 접근권 제한 및 열람 기록 보관\n",
    "# 4 정보의 삭제 또는 반박내용의 게재\n",
    "# 5 정보의 암호화 및 보호\"\"\"\n",
    "\n",
    "# q = \"\"\"정보통신망법 제22조의2에 따라 이동통신단말장치의 운영체제를 제작하여 공급하는 자가 해야 할 조치로 옳은 것은?\n",
    "# 1 이용자의 동의 없이 접근권한 설정\n",
    "# 2 접근권한 철회 기능 구현\n",
    "# 3 접근권한 설정에 대한 이용자 통보 생략\n",
    "# 4 모든 접근권한에 대한 동의 철회 불가\n",
    "# 5 운영체제 내 접근권한 요청 기록을 영구적으로 저장하지 않아야 한다.\"\"\"\n",
    "\n",
    "q = \"\"\"신용정보법 제36조에 따르면, 신용정보주체가 상거래 거절의 근거가 된 신용정보에 대해 이의를 제기할 수 있는 기간은?\n",
    "1 60일 이내\n",
    "2 90일 이내\n",
    "3 30일 이내\n",
    "4 45일 이내\"\"\"\n",
    "\n",
    "print(generate_answer_with_indices(q, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b8c76-b596-440f-8499-ec1a8cb1714d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cecdb-92da-46ba-a123-b264caa704e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722b58e-d526-402a-adc0-6a7bb64d6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b529261-8801-4767-b018-e6661f50f3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b13f45-4284-4eb9-99cf-ea904699ee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1267e-1633-4f42-b984-880c4e5f285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd50a0-f456-413c-ab49-1a662c48bff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12a9fc-cc4d-4390-b988-dde4a24ce55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44abf0a-0235-4eae-95db-1d2b458a7d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602aac4-9c52-4bbc-a961-1029572f1dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
