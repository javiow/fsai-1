{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d3b377-9c86-4e6b-aaef-a8f75fd793dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 435 tensors from ../models/kullm3/KULLM3-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Nlpai Lab KULLM3\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 11B\n",
      "llama_model_loader: - kv   4:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   5:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   6:                  general.base_model.0.name str              = SOLAR 10.7B v1.0\n",
      "llama_model_loader: - kv   7:               general.base_model.0.version str              = v1.0\n",
      "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Upstage\n",
      "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/upstage/SOLAR-...\n",
      "llama_model_loader: - kv  10:                          general.languages arr[str,2]       = [\"en\", \"ko\"]\n",
      "llama_model_loader: - kv  11:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv  12:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q5_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 7.08 GiB (5.66 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      0 '<unk>' is not marked as EOG\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 48\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 34B\n",
      "print_info: model params     = 10.73 B\n",
      "print_info: general.name     = Nlpai Lab KULLM3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  37 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  38 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  39 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  40 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  41 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  42 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  43 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  44 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  45 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  46 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  47 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  48 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 434 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  7245.24 MiB\n",
      "....................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified: layer  34: dev = CPU\n",
      "llama_kv_cache_unified: layer  35: dev = CPU\n",
      "llama_kv_cache_unified: layer  36: dev = CPU\n",
      "llama_kv_cache_unified: layer  37: dev = CPU\n",
      "llama_kv_cache_unified: layer  38: dev = CPU\n",
      "llama_kv_cache_unified: layer  39: dev = CPU\n",
      "llama_kv_cache_unified: layer  40: dev = CPU\n",
      "llama_kv_cache_unified: layer  41: dev = CPU\n",
      "llama_kv_cache_unified: layer  42: dev = CPU\n",
      "llama_kv_cache_unified: layer  43: dev = CPU\n",
      "llama_kv_cache_unified: layer  44: dev = CPU\n",
      "llama_kv_cache_unified: layer  45: dev = CPU\n",
      "llama_kv_cache_unified: layer  46: dev = CPU\n",
      "llama_kv_cache_unified: layer  47: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: size =  768.00 MiB (  4096 cells,  48 layers,  1/1 seqs), K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 3480\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1686\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': '{% if messages[0][\\'role\\'] == \\'system\\' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][\\'content\\'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \"당신은 고려대학교 NLP&AI 연구실에서 만든 AI 챗봇입니다. 당신의 이름은 \\'KULLM\\'으로, 한국어로는 \\'구름\\'을 뜻합니다. 당신은 비도덕적이거나, 성적이거나, 불법적이거나 또는 사회 통념적으로 허용되지 않는 발언은 하지 않습니다. 사용자와 즐겁게 대화하며, 사용자의 응답에 가능한 정확하고 친절하게 응답함으로써 최대한 도와주려고 노력합니다. 질문이 이상하다면, 어떤 부분이 이상한지 설명합니다. 거짓 정보를 발언하지 않도록 주의합니다.\" %}{% endif %}{% for message in loop_messages %}{% if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) %}{{ raise_exception(\\'Conversation roles must alternate user/assistant/user/assistant/...\\') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = \\'<<SYS>>\\\\n\\' + system_message + \\'\\\\n<</SYS>>\\\\n\\\\n\\' + message[\\'content\\'] %}{% else %}{% set content = message[\\'content\\'] %}{% endif %}{% if message[\\'role\\'] == \\'user\\' %}{{ bos_token + \\'[INST] \\' + content.strip() + \\' [/INST]\\'}}{% elif message[\\'role\\'] == \\'system\\' %}{{ \\'<<SYS>>\\\\n\\' + content.strip() + \\'\\\\n<</SYS>>\\\\n\\\\n\\' }}{% elif message[\\'role\\'] == \\'assistant\\' %}{{ \\' \\'  + content.strip() + \\' \\' + eos_token }}{% endif %}{% endfor %}', 'llama.embedding_length': '4096', 'general.base_model.0.repo_url': 'https://huggingface.co/upstage/SOLAR-10.7B-v1.0', 'general.license': 'apache-2.0', 'tokenizer.ggml.add_bos_token': 'true', 'general.size_label': '11B', 'general.type': 'model', 'general.base_model.0.version': 'v1.0', 'llama.attention.head_count_kv': '8', 'general.base_model.0.name': 'SOLAR 10.7B v1.0', 'llama.rope.dimension_count': '128', 'llama.context_length': '4096', 'general.architecture': 'llama', 'general.base_model.0.organization': 'Upstage', 'general.base_model.count': '1', 'llama.feed_forward_length': '14336', 'llama.block_count': '48', 'llama.attention.head_count': '32', 'general.name': 'Nlpai Lab KULLM3', 'tokenizer.ggml.bos_token_id': '1', 'llama.rope.freq_base': '10000.000000', 'general.file_type': '17', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '32000', 'tokenizer.ggml.model': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \"당신은 고려대학교 NLP&AI 연구실에서 만든 AI 챗봇입니다. 당신의 이름은 'KULLM'으로, 한국어로는 '구름'을 뜻합니다. 당신은 비도덕적이거나, 성적이거나, 불법적이거나 또는 사회 통념적으로 허용되지 않는 발언은 하지 않습니다. 사용자와 즐겁게 대화하며, 사용자의 응답에 가능한 정확하고 친절하게 응답함으로써 최대한 도와주려고 노력합니다. 질문이 이상하다면, 어떤 부분이 이상한지 설명합니다. 거짓 정보를 발언하지 않도록 주의합니다.\" %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]'}}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "llama_perf_context_print:        load time =   19427.05 ms\n",
      "llama_perf_context_print: prompt eval time =   19426.86 ms /    77 tokens (  252.30 ms per token,     3.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3469.91 ms /    14 runs   (  247.85 ms per token,     4.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   22905.71 ms /    91 tokens\n",
      "llama_perf_context_print:    graphs reused =         13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "정답: 4. 영업비밀권\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# 모델 로드\n",
    "llm = Llama(\n",
    "    model_path=\"../models/kullm3/KULLM3-Q5_K_M.gguf\",\n",
    "    n_ctx=4096,         # 컨텍스트 윈도우\n",
    "    n_threads=6,        # CPU 스레드 (대회 환경 6 vCPU)\n",
    "    n_gpu_layers=40,    # GPU로 올릴 레이어 수 (4090 VRAM 충분)\n",
    ")\n",
    "\n",
    "# 추론 예시\n",
    "output = llm(\"다음 중 개인정보 보호법 제4조에 따른 정보주체의 권리가 아닌 것은?\\n1. 열람권\\n2. 정정·삭제권\\n3. 처리정지권\\n4. 영업비밀권\", \n",
    "             max_tokens=64, temperature=0.2)\n",
    "\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
