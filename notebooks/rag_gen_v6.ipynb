{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2787c6027a9610f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas tqdm transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db66e65a-3393-4313-b882-823b947f17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/tmp/ipykernel_1868/2559704931.py:72: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d932837261540d08d0896a0479e8f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] ISMS-P 안내서 미발견: /mnt/data/금융보안원 - 금융권에 적합한 ISMS-P 인증기준 점검항목 안내서(2023.12.).pdf\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 623\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] ISMS-P 안내서 미발견: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGUIDE_PDF_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 623\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[OK] built indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(indices\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# 예상 키: 'faiss_hnsw_isms_p_guide', 'faiss_hnsw_all', (...법령 인덱스들)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 303\u001b[0m, in \u001b[0;36mbuild_indices\u001b[0;34m(all_docs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# (b) 글로벌 인덱스\u001b[39;00m\n\u001b[1;32m    301\u001b[0m mat_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings\u001b[38;5;241m.\u001b[39membed_documents([d\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m all_docs]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    302\u001b[0m indices[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_hnsw_all\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbuild_faiss_hnsw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mef_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m: all_docs\n\u001b[1;32m    305\u001b[0m }\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "Cell \u001b[0;32mIn[2], line 107\u001b[0m, in \u001b[0;36mbuild_faiss_hnsw\u001b[0;34m(vectors, m, ef_search)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_faiss_hnsw\u001b[39m(vectors: np\u001b[38;5;241m.\u001b[39mndarray, m: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, ef_search: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexHNSWFlat:\n\u001b[0;32m--> 107\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    108\u001b[0m     idx \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexHNSWFlat(dim, m)\n\u001b[1;32m    109\u001b[0m     idx\u001b[38;5;241m.\u001b[39mhnsw\u001b[38;5;241m.\u001b[39mefSearch \u001b[38;5;241m=\u001b[39m ef_search\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 기존 코드와 완전 호환되는 \"ISMS-P 안내서\" RAG 통합 확장판\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "\n",
    "# ---------------- 사용자 원본 코드 일부 ----------------\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "def is_multiple_choice(question_text):\n",
    "    lines = question_text.strip().split(\"\\n\")\n",
    "    option_count = sum(bool(re.match(r\"^\\s*[1-9][0-9]?\\s\", line)) for line in lines)\n",
    "    return option_count >= 2\n",
    "\n",
    "def extract_question_and_choices(full_text):\n",
    "    lines = full_text.strip().split(\"\\n\")\n",
    "    q_lines, options = [], []\n",
    "    for line in lines:\n",
    "        if re.match(r\"^\\s*[1-9][0-9]?\\s\", line):\n",
    "            options.append(line.strip())\n",
    "        else:\n",
    "            q_lines.append(line.strip())\n",
    "    question = \" \".join(q_lines)\n",
    "    return question, options\n",
    "\n",
    "# ---------------- 모델/임베딩/청킹 파라미터 ----------------\n",
    "LLM_ID = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 경량 추천\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 1200\n",
    "TOP_K = 3\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------- 토크나이저 ----------------\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    # '다.' 등 종결부 + 일반 문장부호 기준 분할\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n",
    "\n",
    "# ---------------- 문서 데이터 구조 ----------------\n",
    "@dataclass\n",
    "class LawDoc:\n",
    "    text: str\n",
    "    meta: Dict\n",
    "\n",
    "# ---------------- 임베딩 ----------------\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,\n",
    "        \"batch_size\": 128,\n",
    "        \"convert_to_numpy\": True,\n",
    "        \"convert_to_tensor\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# ---------------- 공통 유틸 ----------------\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for p in reader.pages:\n",
    "        t = p.extract_text() or \"\"\n",
    "        text += t + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def normalize_common(text: str) -> str:\n",
    "    # circled numbers → (n)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    # 한자 제거(있으면)\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    # 공백 정리\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\s+\\n', '\\n', text)\n",
    "    text = re.sub(r'\\n\\s+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 32) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    idx = faiss.IndexHNSWFlat(dim, m)\n",
    "    idx.hnsw.efSearch = ef_search\n",
    "    idx.add(vectors.astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "def chunk_by_tokens(prefix: str, body: str) -> List[str]:\n",
    "    prefix = (prefix or \"\").strip()\n",
    "    sents = split_sentences_ko(body) or [body]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix + \"\\n\") if prefix else 0\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > CHUNK_TOKENS and cur:\n",
    "            chunks.append((prefix + \"\\n\" if prefix else \"\") + \" \".join(cur))\n",
    "            keep = cur[-1] if CHUNK_OVERLAP > 0 and cur else \"\"\n",
    "            cur = [keep] if keep else []\n",
    "            cur_toks = (token_len(prefix + \"\\n\") if prefix else 0) + (token_len(keep) if keep else 0)\n",
    "        cur.append(s); cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append((prefix + \"\\n\" if prefix else \"\") + \" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "# =====================================================================\n",
    "# 1) \"법령\" 파트 (기존): 조문 단위 분리 및 인덱싱 그대로 유지\n",
    "# =====================================================================\n",
    "LAW_CONFIG = {\n",
    "    # (예시) 개인정보 보호법 등 기존에 쓰던 항목들...\n",
    "    # 필요 시 그대로 유지/사용. 여기서는 ISMS-P 추가가 목적이므로 생략 가능.\n",
    "}\n",
    "\n",
    "ARTICLE_HEADER_PATTERN = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'\n",
    "def split_articles(raw_text: str) -> List[Tuple[str, str, str]]:\n",
    "    parts = re.split(ARTICLE_HEADER_PATTERN, raw_text)\n",
    "    out = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m:\n",
    "            continue\n",
    "        article_id = m.group(1)\n",
    "        article_title = m.group(2)\n",
    "        out.append((article_id, article_title, body))\n",
    "    return out\n",
    "\n",
    "def clean_text_by_config(text: str, drop_patterns: List[str]) -> str:\n",
    "    for pat in drop_patterns:\n",
    "        text = re.sub(pat, '', text)\n",
    "    return normalize_common(text)\n",
    "\n",
    "def preprocess_law(law_id: str, cfg: Dict) -> List[LawDoc]:\n",
    "    raw = load_pdf_text(cfg[\"pdf_path\"])\n",
    "    cleaned = clean_text_by_config(raw, cfg[\"drop_patterns\"])\n",
    "    articles = split_articles(cleaned)\n",
    "\n",
    "    docs: List[LawDoc] = []\n",
    "    effective_date = None\n",
    "    for article_id, title, body in articles:\n",
    "        header = f'{cfg[\"law_name\"]} {article_id}({title})'\n",
    "        chunks = chunk_by_tokens(header, body)\n",
    "        for ch in chunks:\n",
    "            meta = {\n",
    "                \"law_id\": law_id,\n",
    "                \"law_name\": cfg[\"law_name\"],\n",
    "                \"article_id\": article_id,\n",
    "                \"article_title\": title,\n",
    "                \"effective_date\": effective_date,\n",
    "                \"tok_len\": token_len(ch),\n",
    "                \"source_uri\": cfg.get(\"pdf_path\"),\n",
    "                \"version\": None\n",
    "            }\n",
    "            docs.append(LawDoc(text=ch, meta=meta))\n",
    "    return docs\n",
    "\n",
    "# =====================================================================\n",
    "# 2) \"금융보안원 ISMS-P 점검항목 안내서\" 전용 전처리/청킹/인덱싱\n",
    "# =====================================================================\n",
    "\n",
    "# 업로드된 파일 경로(필요 시 환경에 맞게 변경)\n",
    "GUIDE_PDF_PATH = \"/mnt/data/금융보안원 - 금융권에 적합한 ISMS-P 인증기준 점검항목 안내서(2023.12.).pdf\"\n",
    "\n",
    "# 안내서에서 반복되는 머리말/꼬리말/페이지 번호/표 캡션 등을 최대한 제거\n",
    "ISMSP_DROP_PATTERNS = [\n",
    "    r'금융보안원', r'ISMS-?P', r'인증기준', r'점검항목 안내서',\n",
    "    r'목차\\s*', r'표\\s*\\d+[-.]?\\d*', r'그림\\s*\\d+[-.]?\\d*',\n",
    "    r'페이지\\s*\\d+/\\d+', r'Page\\s*\\d+/\\d+',\n",
    "    r'^\\s*\\d+\\s*$',              # 단독 페이지 번호 라인\n",
    "]\n",
    "\n",
    "# (헤딩) 섹션 탐지: \"1.\", \"1.1\", \"Ⅰ.\", \"가.\", \"(1)\" 등 다양하게 오는 경우를 폭넓게 커버\n",
    "SECTION_HEADER = re.compile(\n",
    "    r'^\\s*((?:[IVX]+\\.|\\d+(?:\\.\\d+)*\\.?|[가-힣]\\.|\\( ?\\d+\\)|\\( ?[가-힣]\\)))\\s+(.{2,100})\\s*$'\n",
    ")\n",
    "\n",
    "def clean_isms_p_text(raw_text: str) -> str:\n",
    "    # 줄 단위로 header/footer/표 번호/빈 줄 제거\n",
    "    lines = [ln for ln in raw_text.splitlines()]\n",
    "    cleaned_lines = []\n",
    "    for ln in lines:\n",
    "        skip = False\n",
    "        for pat in ISMSP_DROP_PATTERNS:\n",
    "            if re.search(pat, ln, flags=re.IGNORECASE):\n",
    "                # 'ISMS-P' 키워드가 본문 내용인 경우까지 지우지 않도록, 전역 제거 대신 \"머리말 패턴\" 중심\n",
    "                # 다만 본문 키워드까지 과하게 지워진다고 느껴지면 이 블록을 보수적으로 조정하세요.\n",
    "                pass\n",
    "        # 너무 짧은 잡음 라인/구분선 제거\n",
    "        if re.match(r'^\\s*[-=]{4,}\\s*$', ln): \n",
    "            continue\n",
    "        if re.match(r'^\\s*$', ln):\n",
    "            continue\n",
    "        cleaned_lines.append(ln)\n",
    "\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "    # 페이지 머리/꼬리 흔적 제거(보수적으로)\n",
    "    text = re.sub(r'\\n?Copyright .*?\\n', '\\n', text, flags=re.IGNORECASE)\n",
    "    text = normalize_common(text)\n",
    "    return text\n",
    "\n",
    "def split_isms_p_sections(clean_text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    안내서의 특성상 '섹션 헤더 라인'을 기준으로 큰 블록을 만든 뒤,\n",
    "    블록 내부는 문장 단위 청킹으로 토크나이즈 예산에 맞춰 분할합니다.\n",
    "    반환: [(섹션제목, 섹션 본문), ...]\n",
    "    \"\"\"\n",
    "    lines = clean_text.split(\"\\n\")\n",
    "    sections = []\n",
    "    cur_title, cur_buf = None, []\n",
    "\n",
    "    def push():\n",
    "        nonlocal sections, cur_title, cur_buf\n",
    "        if cur_title and cur_buf:\n",
    "            body = \" \".join(cur_buf).strip()\n",
    "            if body:\n",
    "                sections.append((cur_title, body))\n",
    "        cur_title, cur_buf = None, []\n",
    "\n",
    "    for ln in lines:\n",
    "        m = SECTION_HEADER.match(ln)\n",
    "        if m:\n",
    "            # 새로운 섹션 시작\n",
    "            push()\n",
    "            # 제목: \"1. 개요\" 같은 형태로 정규화\n",
    "            mark, title = m.group(1), m.group(2)\n",
    "            cur_title = f\"{mark} {title}\".strip()\n",
    "        else:\n",
    "            if cur_title is None:\n",
    "                # 서문/요약 같은 프리앰블은 '0. 서문' 식으로 묶어줌\n",
    "                cur_title = \"서문\"\n",
    "            cur_buf.append(ln)\n",
    "\n",
    "    push()\n",
    "    return sections\n",
    "\n",
    "def preprocess_isms_p_guide(pdf_path: str) -> List[LawDoc]:\n",
    "    raw = load_pdf_text(pdf_path)\n",
    "    cleaned = clean_isms_p_text(raw)\n",
    "    sections = split_isms_p_sections(cleaned)\n",
    "\n",
    "    docs: List[LawDoc] = []\n",
    "    for title, body in sections:\n",
    "        # 섹션을 먼저 큰 덩어리로 만들고, 토큰 청킹\n",
    "        chunks = chunk_by_tokens(f\"ISMS-P 점검항목 안내서 {title}\", body)\n",
    "        for ch in chunks:\n",
    "            meta = {\n",
    "                \"law_id\": \"isms_p_guide\",\n",
    "                \"law_name\": \"금융보안원 ISMS-P 점검항목 안내서\",\n",
    "                \"section_title\": title,\n",
    "                \"tok_len\": token_len(ch),\n",
    "                \"source_uri\": pdf_path,\n",
    "                \"doc_type\": \"guide\",\n",
    "            }\n",
    "            docs.append(LawDoc(text=ch, meta=meta))\n",
    "    return docs\n",
    "\n",
    "# =====================================================================\n",
    "# 3) 인덱스 빌드 (법령 + ISMS-P 안내서)\n",
    "# =====================================================================\n",
    "\n",
    "def build_indices(all_docs: List[LawDoc]):\n",
    "    # (a) 문서군별 인덱스\n",
    "    per_group_docs: Dict[str, List[LawDoc]] = {}\n",
    "    for d in all_docs:\n",
    "        group = d.meta.get(\"law_id\", \"misc\")\n",
    "        per_group_docs.setdefault(group, []).append(d)\n",
    "\n",
    "    indices = {}\n",
    "    for gid, docs in per_group_docs.items():\n",
    "        mat = np.array(embeddings.embed_documents([d.text for d in docs]), dtype=np.float32)\n",
    "        indices[f\"faiss_hnsw_{gid}\"] = {\n",
    "            \"index\": build_faiss_hnsw(mat, m=32, ef_search=32),\n",
    "            \"docs\": docs\n",
    "        }\n",
    "\n",
    "    # (b) 글로벌 인덱스\n",
    "    mat_all = np.array(embeddings.embed_documents([d.text for d in all_docs]), dtype=np.float32)\n",
    "    indices[\"faiss_hnsw_all\"] = {\n",
    "        \"index\": build_faiss_hnsw(mat_all, m=32, ef_search=32),\n",
    "        \"docs\": all_docs\n",
    "    }\n",
    "    return indices\n",
    "\n",
    "# ---------------- 라우팅 규칙 업데이트 ----------------\n",
    "ARTICLE_PTRN = re.compile(r\"제\\d+조(?:의\\d+)?\")\n",
    "\n",
    "LAW_HINTS = {\n",
    "    # 개인정보 보호법\n",
    "    \"pipa\": {\n",
    "        \"law_name\": \"개인정보 보호법\",\n",
    "        \"pdf_path\": \"../data/개인정보 보호법(법률)(제19234호)(20250313).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터',\n",
    "            r'국가법령정보센터\\s*개인정보\\s*보호법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>',         # <개정 …>, <신설 …>\n",
    "            r'\\[[^\\]]+\\]',      # [본조신설 …]\n",
    "        ],\n",
    "    },\n",
    "    # 신용정보의 이용 및 보호에 관한 법률\n",
    "    \"ciupa\": {\n",
    "        \"law_name\": \"신용정보법\",\n",
    "        \"pdf_path\": \"../data/신용정보의 이용 및 보호에 관한 법률(법률)(제20304호)(20240814).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*신용정보.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자서명법\n",
    "    \"es_act\": {\n",
    "        \"law_name\": \"전자서명법\",\n",
    "        \"pdf_path\": \"../data/전자서명법(법률)(제18479호)(20221020).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자서명법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 정보통신망 이용촉진 및 정보보호 등에 관한 법률\n",
    "    \"icn_act\": {\n",
    "        \"law_name\": \"정보통신망법\",\n",
    "        \"pdf_path\": \"../data/정보통신망 이용촉진 및 정보보호 등에 관한 법률(법률)(제20678호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*정보통신망.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자금융거래법\n",
    "    \"eft_act\": {\n",
    "        \"law_name\": \"전자금융거래법\",\n",
    "        \"pdf_path\": \"../data/전자금융거래법(법률)(제19734호)(20240915).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자금융거래.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 전자금융감독규정\n",
    "    \"rs_act\": {\n",
    "        \"law_name\": \"전자금융감독규정\",\n",
    "        \"pdf_path\": \"../data/전자금융감독규정(금융위원회고시)(제2025-4호)(20250205).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*전자금융거래.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "    # 자본시장법\n",
    "    \"fis_act\": {\n",
    "        \"law_name\": \"자본시장법\",\n",
    "        \"pdf_path\": \"../data/자본시장과 금융투자업에 관한 법률(법률)(제20718호)(20250722).pdf\",\n",
    "        \"drop_patterns\": [\n",
    "            r'법제처\\s+\\d+\\s+국가법령정보센터\\s*자본시장.*법',\n",
    "            r'법제처|국가법령정보센터',\n",
    "            r'<[^>]+>', r'\\[[^\\]]+\\]',\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "LAW_HINTS = {\n",
    "    # 기존 법령 힌트들(있다면 유지) ...\n",
    "    # \"pipa\": (\"개인정보\", \"개인정보보호법\"),\n",
    "    # ...\n",
    "    # ISMS-P 안내서 힌트 추가\n",
    "    \"isms_p_guide\": (\n",
    "        \"isms p\", \"isms-p\", \"ismsp\", \"ismsp\",\n",
    "        \"인증기준\", \"점검항목\", \"관리적 보안\", \"기술적 보안\", \"물리적 보안\",\n",
    "        \"보안성 심의\", \"접근통제\", \"암호\", \"취약점 진단\", \"로그\", \"백업\",\n",
    "        \"금융보안원\", \"개인정보 관리체계\", \"관리체계 수립\", \"개선 조치\", \"보호대책\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def detect_law_id(query: str) -> Optional[str]:\n",
    "    q = query.lower()\n",
    "    for law_id, kws in LAW_HINTS.items():\n",
    "        if any(kw.lower() in q for kw in kws):\n",
    "            return law_id\n",
    "    return None\n",
    "\n",
    "def route_is_domain(query: str) -> bool:\n",
    "    domain_kws = (\"법\", \"조(\", \"과징금\", \"처벌\", \"보안\", \"침해\", \"금융\", \"개인정보\", \"신용정보\",\n",
    "                  \"전자서명\", \"정보통신\", \"전자금융\", \"금융감독\", \"자본\", \"자본시장\", \"투자\",\n",
    "                  \"점검항목\", \"인증기준\", \"관리체계\", \"ISMS\")\n",
    "    q = query.lower()\n",
    "    return any(kw.lower() in q for kw in domain_kws) or bool(ARTICLE_PTRN.search(query))\n",
    "\n",
    "def choose_index(indices: dict, query: str):\n",
    "    law_id = detect_law_id(query)\n",
    "    if law_id:\n",
    "        key = f\"faiss_hnsw_{law_id}\"\n",
    "        if key in indices:\n",
    "            return indices[key]\n",
    "    if route_is_domain(query) and \"faiss_hnsw_all\" in indices:\n",
    "        return indices[\"faiss_hnsw_all\"]\n",
    "    return None\n",
    "\n",
    "# ---------------- 컨텍스트 패킹/프롬프트/생성(사용자 코드 유지) ----------------\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n",
    "\n",
    "def chat_prompt(system, user):\n",
    "    messages = [{\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": user}]\n",
    "    return llm_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 96 if opt_cnt >= 2 else 192\n",
    "\n",
    "def build_prompt(query: str, use_context: bool, context) -> Tuple[str, int]:\n",
    "    if is_multiple_choice(query):\n",
    "        question, options = extract_question_and_choices(query)\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{context}\\n=== 끝 ===\\n\"\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n선택지:\\n{chr(10).join(options)}\\n\\n답변:\"\n",
    "            )\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "            return prompt, 3072\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"아래 질문에 대해 적절한 **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
    "                f\"질문: {question}\\n선택지:\\n{chr(10).join(options)}\\n\\n답변:\"\n",
    "            )\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "            return prompt, 2048\n",
    "    else:\n",
    "        if use_context:\n",
    "            prompt = (\n",
    "                \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "                f\"=== 컨텍스트 ===\\n{context}\\n=== 끝 ===\\n\"\n",
    "                \"아래 질문에 대해 **사실에 근거한 간결한 답변**을 작성하세요.\\n\\n\"\n",
    "                \"규칙:\\n\"\n",
    "                \"1. 답변은 2~3문장 이내로 작성합니다. 장황한 서론, 결론 문구는 쓰지 않습니다.\\n\"\n",
    "                \"2. 불확실할 경우 '알 수 없습니다'라고 답하고 생성을 종료합니다.\\n\"\n",
    "                \"3. 특수문자 없이 오로지 한글과 숫자로만 대답합니다.\\n\"\n",
    "                f\"질문: {query}답변:\"\n",
    "            )\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "            return prompt, 3072\n",
    "        else:\n",
    "            prompt = (\n",
    "                \"아래 질문에 대해 **사실에 근거한 간결한 답변**을 작성하세요.\\n\\n\"\n",
    "                \"규칙:\\n\"\n",
    "                \"1. 답변은 2~3문장 이내로 작성합니다. 장황한 서론, 결론 문구는 쓰지 않습니다.\\n\"\n",
    "                \"2. 불확실할 경우 '알 수 없습니다'라고 답하고 생성을 종료합니다.\\n\"\n",
    "                \"3. 특수문자 없이 오로지 한글과 숫자로만 대답합니다.\\n\"\n",
    "                f\"질문: {query}답변:\"\n",
    "            )\n",
    "            system = \"당신은 금융/보안 QA 도우미입니다. 포맷을 엄격히 지키세요.\"\n",
    "            prompt = chat_prompt(system, prompt)\n",
    "            return prompt, 2048\n",
    "\n",
    "def faiss_search_with_scores_from_index(index_entry: dict, query: str, top_k: int = TOP_K):\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index_entry[\"index\"].search(qv, top_k)    # L2\n",
    "    cos = 1.0 - (D[0] / 2.0)                         # L2 → cosine (정규화 가정)\n",
    "    out = []\n",
    "    docs = index_entry[\"docs\"]\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        ii = int(i)\n",
    "        if ii >= 0:\n",
    "            out.append((docs[ii], float(cos[idx])))\n",
    "    return out\n",
    "\n",
    "def generate_answer_with_indices(query: str, indices: dict) -> str:\n",
    "    if not route_is_domain(query):\n",
    "        prompt, max_len = build_prompt(query, use_context=False, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.0, top_p=0.1,\n",
    "                                     repetition_penalty=1.1,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    idx_entry = choose_index(indices, query)\n",
    "    if idx_entry is None:\n",
    "        prompt, max_len = build_prompt(query, use_context=False, context=None)\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.0, top_p=0.1,\n",
    "                                     repetition_penalty=1.1,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    scored = faiss_search_with_scores_from_index(idx_entry, query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "    THRESH = 0.80\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "    context = pack_context([d for d, s in scored if s >= THRESH], token_budget=CTX_TOKEN_BUDGET) if use_context else None\n",
    "    prompt, max_len = build_prompt(query, use_context, context)\n",
    "\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=dynamic_max_new_tokens(query),\n",
    "            do_sample=False, temperature=0.0, top_p=0.1,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "# ---------------- LLM 로드 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ---------------- 후처리(원본 유지) ----------------\n",
    "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
    "    if \"답변:\" in generated_text:\n",
    "        text = generated_text.split(\"답변:\")[-1].strip()\n",
    "    else:\n",
    "        text = generated_text.strip()\n",
    "    if not text:\n",
    "        return \"미응답\"\n",
    "    if is_multiple_choice(original_question):\n",
    "        match = re.match(r\"\\D*([1-9][0-9]?)\", text)\n",
    "        return match.group(1) if match else \"0\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def strip_explanations(answer: str) -> str:\n",
    "    lines = answer.split(\"\\n\")\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        m = re.match(r\"^\\s*[-0-9.]*\\s*([^\\:]+)\", line)\n",
    "        if m:\n",
    "            keyword = m.group(1).strip()\n",
    "            cleaned.append(keyword)\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "# ---------------- 인덱스 구축: 법령(있으면) + ISMS-P 안내서 ----------------\n",
    "all_docs: List[LawDoc] = []\n",
    "\n",
    "# (선택) 기존 LAW_CONFIG 처리\n",
    "for law_id, cfg in LAW_CONFIG.items():\n",
    "    if not os.path.exists(cfg[\"pdf_path\"]):\n",
    "        print(f\"[WARN] PDF not found: {cfg['pdf_path']}\")\n",
    "        continue\n",
    "    docs = preprocess_law(law_id, cfg)\n",
    "    all_docs.extend(docs)\n",
    "    print(f\"[OK] {cfg['law_name']} → chunks: {len(docs)}\")\n",
    "\n",
    "# ISMS-P 안내서 처리\n",
    "if os.path.exists(GUIDE_PDF_PATH):\n",
    "    isms_docs = preprocess_isms_p_guide(GUIDE_PDF_PATH)\n",
    "    all_docs.extend(isms_docs)\n",
    "    print(f\"[OK] ISMS-P 안내서 → chunks: {len(isms_docs)}\")\n",
    "else:\n",
    "    print(f\"[WARN] ISMS-P 안내서 미발견: {GUIDE_PDF_PATH}\")\n",
    "\n",
    "indices = build_indices(all_docs)\n",
    "print(\"[OK] built indices:\", list(indices.keys()))\n",
    "# 예상 키: 'faiss_hnsw_isms_p_guide', 'faiss_hnsw_all', (...법령 인덱스들)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
