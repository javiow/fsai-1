{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c68819-0aaa-42aa-90e3-577ff79af9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu==1.7.2\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu==1.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e868aee8-9b47-40a6-a3fd-15b1dbe699ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn==2.5.8\n",
      "  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.5.8) (2.1.0+cu118)\n",
      "Collecting einops (from flash-attn==2.5.8)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.5.8) (23.2)\n",
      "Collecting ninja (from flash-attn==2.5.8)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (4.14.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.5.8) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.5.8) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==2.5.8) (1.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=121735960 sha256=f7b1002a954d42f6451c812d1ab69a0f70092e447df49b6dd384b2006f9094ee\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: ninja, einops, flash-attn\n",
      "Successfully installed einops-0.8.1 flash-attn-2.5.8 ninja-1.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn==2.5.8 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bc6bb1-69e6-48ff-958d-f2f4f5a38ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# KULLM3 + FAISS(HNSW) 빠른 RAG (고정폭 lookbehind 수정 포함)\n",
    "\n",
    "import os, re, json, math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PDF_PATH = \"../data/개인정보 보호법.pdf\"\n",
    "LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 대체: intfloat/multilingual-e5-small\n",
    "TOP_K = 4\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 900\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 0) 경량 라우터 (정규식/키워드)\n",
    "LAW_KWS = (\"개인정보\", \"제\", \"조(\", \"시행령\", \"과징금\", \"처벌\", \"보안\", \"금융\", \"증권\", \"PPI\", \"CPI\", \"자본시장법\")\n",
    "def route_is_law(query: str) -> bool:\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in LAW_KWS)\n",
    "\n",
    "# ---------------- PDF 로드 & 정제 ----------------\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def clean_article_text(text: str) -> str:\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터', '', text)\n",
    "    text = re.sub(r'국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처|국가법령정보센터', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text() or \"\"\n",
    "        full_text += t + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def split_articles(text: str) -> List[Tuple[str, str, str]]:\n",
    "    pattern = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'\n",
    "    parts = re.split(pattern, text)\n",
    "    results = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m: \n",
    "            continue\n",
    "        art_id = m.group(1)\n",
    "        title = m.group(2)\n",
    "        results.append((art_id, title, clean_article_text(body)))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914873a0-d4d8-44f9-9fd8-0c25da8f84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 토큰 기준 청킹 ----------------\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    lookbehind 고정폭으로 분리:\n",
    "    - '...다.' 패턴 뒤 공백\n",
    "    - 일반 종결부호(. ? ! 。 ！ ？) 뒤 공백\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n",
    "\n",
    "def chunk_by_tokens(text: str, header: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(text)\n",
    "    if not sents:\n",
    "        sents = [text]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > max_tokens and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            if overlap > 0:\n",
    "                keep = cur[-1] if cur else \"\"\n",
    "                cur = [keep] if keep else []\n",
    "                cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "            else:\n",
    "                cur, cur_toks = [], token_len(prefix)\n",
    "        cur.append(s)\n",
    "        cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82393c99-d378-449a-b4a8-edc5639378b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1541/4251994564.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 임베딩 & FAISS(HNSW) ----------------\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np, faiss\n",
    "from dataclasses import dataclass\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 128,\n",
    "                   \"convert_to_numpy\": True, \"convert_to_tensor\": False}\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    text: str\n",
    "    meta: dict\n",
    "\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 64) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(dim, m)\n",
    "    index.hnsw.efSearch = ef_search\n",
    "    index.add(vectors.astype(np.float32))\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2288fa93-7918-489b-9678-f821b3942121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 데이터 준비 ----------------\n",
    "full_text = load_pdf_text(PDF_PATH)\n",
    "articles = split_articles(full_text)\n",
    "\n",
    "docs: List[Doc] = []\n",
    "for art_id, title, body in articles:\n",
    "    header = f\"개인정보 보호법 {art_id}({title})\"\n",
    "    for ch in chunk_by_tokens(body, header, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP):\n",
    "        docs.append(Doc(text=ch, meta={\n",
    "            \"article\": art_id, \"title\": title, \"tok_len\": token_len(ch)  # ← 캐시\n",
    "        }))\n",
    "\n",
    "corpus_texts = [d.text for d in docs]\n",
    "emb_matrix = np.array(embeddings.embed_documents(corpus_texts), dtype=np.float32)  # (N, D)\n",
    "\n",
    "# index = build_faiss_hnsw(emb_matrix, m=32, ef_search=64)\n",
    "index = build_faiss_hnsw(emb_matrix, m=32, ef_search=32)  # ← 64 -> 32 (보통 절반 가까이 빨라짐)\n",
    "\n",
    "\n",
    "def faiss_search(query: str, top_k: int = TOP_K) -> List[Doc]:\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    return [docs[int(i)] for i in I[0] if int(i) >= 0]\n",
    "\n",
    "# pack_context에서 캐시 활용 + 잘라붙일 때만 토크나이즈\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:  # 혹시 없는 경우만 계산\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4e5fed-0733-47c3-bb2c-a4383b467e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89059e3604714a66a2ea0acd60d9f2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x77d3cfc648b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e61a59-701b-4c05-9af7-913f9622c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 텍스트 생성 ----------------\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 128 if opt_cnt >= 2 else 256\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 한국 법령, 금융, 보안 도메인 Q/A를 담당하는 도우미입니다. \"\n",
    "    \"아는 사실만 간결하게 답하고, 모르면 '알 수 없습니다'라고 말하세요.\"\n",
    ")\n",
    "USER_TPL = (\n",
    "    \"다음 컨텍스트만 사용해 한국어로 정확하게 답하세요.\\n\"\n",
    "    \"===\\n{context}\\n===\\n질문: {query}\"\n",
    ")\n",
    "\n",
    "def faiss_search_with_scores(query: str, top_k: int = TOP_K):\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)              # L2 거리 (정규화 벡터)\n",
    "    cos = 1.0 - (D[0] / 2.0)                    # L2 -> cosine\n",
    "    out = []\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        if int(i) >= 0:\n",
    "            out.append((docs[int(i)], float(cos[idx])))\n",
    "    return out\n",
    "\n",
    "def generate_answer(query: str) -> str:\n",
    "    # (0) 라우팅: 법/금융/보안 질의가 아니면 검색 자체를 생략 → 즉시 베이스모델\n",
    "    if not route_is_law(query):\n",
    "        prompt = (\n",
    "            \"당신은 한국어로 간결하고 정확하게 답하는 도우미입니다. \"\n",
    "            \"사실에 근거해 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # (1) 벡터 검색 + 점수\n",
    "    scored = faiss_search_with_scores(query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # (2) 임계치(튜닝 포인트): 0.65 → 0.70로 올리면 컨텍스트 사용 빈도↓ → 평균 지연↓\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    # (3) 컨텍스트 조립 (캐시된 tok_len 사용)\n",
    "    ctx = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else \"\"\n",
    "\n",
    "    # (4) 프롬프트 구성 (불필요한 장식 최소화)\n",
    "    if use_context:\n",
    "        prompt = (\n",
    "            \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "            f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        max_new = dynamic_max_new_tokens(query)\n",
    "        max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"당신은 한국 법령·금융·보안 Q/A 도우미입니다. \"\n",
    "            \"사실에 근거해 간결히 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        max_new = dynamic_max_new_tokens(query)\n",
    "        max_len = 2048\n",
    "\n",
    "    # (5) 토크나이즈/생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a33789-82f3-44d7-a509-a6a1e8d2eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"개인정보보호법 제22조의2에 따라 만 14세 미만 아동의 개인정보를 처리하기 위해 필요한 절차로 옳은 것은?\n",
    "1 아동의 학교의 동의를 받아야 한다.\n",
    "2 법정대리인의 동의를 받아야 한다.\n",
    "3 아동 본인의 동의만 받으면 된다.\n",
    "4 아동의 친구의 동의를 받아야 한다.\"\"\"\n",
    "print(generate_answer(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e22577-9c4d-4e80-be0f-6032ccd53e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b8c76-b596-440f-8499-ec1a8cb1714d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cecdb-92da-46ba-a123-b264caa704e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722b58e-d526-402a-adc0-6a7bb64d6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b529261-8801-4767-b018-e6661f50f3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b13f45-4284-4eb9-99cf-ea904699ee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1267e-1633-4f42-b984-880c4e5f285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd50a0-f456-413c-ab49-1a662c48bff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12a9fc-cc4d-4390-b988-dde4a24ce55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44abf0a-0235-4eae-95db-1d2b458a7d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602aac4-9c52-4bbc-a961-1029572f1dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
