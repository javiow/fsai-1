{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99bc6bb1-69e6-48ff-958d-f2f4f5a38ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# KULLM3 + FAISS(HNSW) 빠른 RAG (고정폭 lookbehind 수정 포함)\n",
    "\n",
    "import os, re, json, math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PDF_PATH = \"../data/개인정보 보호법.pdf\"\n",
    "LLM_ID = \"nlpai-lab/KULLM3\"\n",
    "EMB_MODEL = \"jhgan/ko-sroberta-multitask\"   # 대체: intfloat/multilingual-e5-small\n",
    "TOP_K = 4\n",
    "CHUNK_TOKENS = 600\n",
    "CHUNK_OVERLAP = 32\n",
    "CTX_TOKEN_BUDGET = 900\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 0) 경량 라우터 (정규식/키워드)\n",
    "LAW_KWS = (\"개인정보\", \"제\", \"조(\", \"시행령\", \"과징금\", \"처벌\", \"보안\", \"금융\", \"증권\", \"PPI\", \"CPI\", \"자본시장법\")\n",
    "def route_is_law(query: str) -> bool:\n",
    "    q = query.lower()\n",
    "    return any(kw in q for kw in LAW_KWS)\n",
    "\n",
    "# ---------------- PDF 로드 & 정제 ----------------\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def clean_article_text(text: str) -> str:\n",
    "    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처\\s+\\d+\\s+국가법령정보센터', '', text)\n",
    "    text = re.sub(r'국가법령정보센터\\s*개인정보\\s*보호법', '', text)\n",
    "    text = re.sub(r'법제처|국가법령정보센터', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\[[^\\]]+\\]', '', text)\n",
    "    circled = '①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮⑯⑰⑱⑲⑳'\n",
    "    for idx, c in enumerate(circled, 1):\n",
    "        text = text.replace(c, f'({idx})')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    return text\n",
    "\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text() or \"\"\n",
    "        full_text += t + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def split_articles(text: str) -> List[Tuple[str, str, str]]:\n",
    "    pattern = r'(제\\d+조(?:의\\d+)?\\([^)]+\\))'\n",
    "    parts = re.split(pattern, text)\n",
    "    results = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i]\n",
    "        body = (parts[i+1] if i+1 < len(parts) else \"\").strip().replace(\"\\n\", \" \")\n",
    "        m = re.match(r'(제\\d+조(?:의\\d+)?)[(]([^)]+)[)]', header)\n",
    "        if not m: \n",
    "            continue\n",
    "        art_id = m.group(1)\n",
    "        title = m.group(2)\n",
    "        results.append((art_id, title, clean_article_text(body)))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914873a0-d4d8-44f9-9fd8-0c25da8f84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c148d084f75442b08e5a61e17243cc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ae82844ccf46b6b8661d3b026378da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199eaeebbca64b48848ca92dac732cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b970e7d91f4ff2ac065c6a0037d20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- 토큰 기준 청킹 ----------------\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_ID)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(llm_tokenizer(s, add_special_tokens=False)[\"input_ids\"])\n",
    "\n",
    "def split_sentences_ko(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    lookbehind 고정폭으로 분리:\n",
    "    - '...다.' 패턴 뒤 공백\n",
    "    - 일반 종결부호(. ? ! 。 ！ ？) 뒤 공백\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return re.split(r'(?<=다\\.)\\s+|(?<=[.?!。！？])\\s+', text)\n",
    "\n",
    "def chunk_by_tokens(text: str, header: str, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    prefix = header.strip() + \"\\n\"\n",
    "    sents = split_sentences_ko(text)\n",
    "    if not sents:\n",
    "        sents = [text]\n",
    "    chunks, cur, cur_toks = [], [], token_len(prefix)\n",
    "    for s in sents:\n",
    "        tl = token_len(s)\n",
    "        if cur_toks + tl > max_tokens and cur:\n",
    "            chunks.append(prefix + \" \".join(cur))\n",
    "            if overlap > 0:\n",
    "                keep = cur[-1] if cur else \"\"\n",
    "                cur = [keep] if keep else []\n",
    "                cur_toks = token_len(prefix) + (token_len(keep) if keep else 0)\n",
    "            else:\n",
    "                cur, cur_toks = [], token_len(prefix)\n",
    "        cur.append(s)\n",
    "        cur_toks += tl\n",
    "    if cur:\n",
    "        chunks.append(prefix + \" \".join(cur))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82393c99-d378-449a-b4a8-edc5639378b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_799/4251994564.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fcd3195da841c781273b7b14149c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b187d18659df476d8e811118f9e3ed22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b2b84e2b3c46f59af5fc46a4d216b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42f6768fb3e46c7885828ffc9e1be68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987aca196de14ca09521b4da3e5b18f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f5838a5cc047c4afe5bbd8beb695e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd6b2070b2e49c987572af46cbae3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9130b5b24b4c1ca650beb700ce15fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00da789bb8a54aca87a6d5eef1869f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a0c6992ce54b6b8b9d5ff7680f465a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d09ffc7db1041d98fa6966ee8d8b788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- 임베딩 & FAISS(HNSW) ----------------\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np, faiss\n",
    "from dataclasses import dataclass\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMB_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 128,\n",
    "                   \"convert_to_numpy\": True, \"convert_to_tensor\": False}\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    text: str\n",
    "    meta: dict\n",
    "\n",
    "def build_faiss_hnsw(vectors: np.ndarray, m: int = 32, ef_search: int = 64) -> faiss.IndexHNSWFlat:\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(dim, m)\n",
    "    index.hnsw.efSearch = ef_search\n",
    "    index.add(vectors.astype(np.float32))\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2288fa93-7918-489b-9678-f821b3942121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 데이터 준비 ----------------\n",
    "full_text = load_pdf_text(PDF_PATH)\n",
    "articles = split_articles(full_text)\n",
    "\n",
    "docs: List[Doc] = []\n",
    "for art_id, title, body in articles:\n",
    "    header = f\"개인정보 보호법 {art_id}({title})\"\n",
    "    for ch in chunk_by_tokens(body, header, max_tokens=CHUNK_TOKENS, overlap=CHUNK_OVERLAP):\n",
    "        docs.append(Doc(text=ch, meta={\n",
    "            \"article\": art_id, \"title\": title, \"tok_len\": token_len(ch)  # ← 캐시\n",
    "        }))\n",
    "\n",
    "corpus_texts = [d.text for d in docs]\n",
    "emb_matrix = np.array(embeddings.embed_documents(corpus_texts), dtype=np.float32)  # (N, D)\n",
    "\n",
    "# index = build_faiss_hnsw(emb_matrix, m=32, ef_search=64)\n",
    "index = build_faiss_hnsw(emb_matrix, m=32, ef_search=32)  # ← 64 -> 32 (보통 절반 가까이 빨라짐)\n",
    "\n",
    "\n",
    "def faiss_search(query: str, top_k: int = TOP_K) -> List[Doc]:\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    return [docs[int(i)] for i in I[0] if int(i) >= 0]\n",
    "\n",
    "# pack_context에서 캐시 활용 + 잘라붙일 때만 토크나이즈\n",
    "def pack_context(docs_in, token_budget=CTX_TOKEN_BUDGET):\n",
    "    acc, used = [], 0\n",
    "    for d in docs_in:\n",
    "        tl = d.meta.get(\"tok_len\", None)\n",
    "        if tl is None:  # 혹시 없는 경우만 계산\n",
    "            tl = token_len(d.text); d.meta[\"tok_len\"] = tl\n",
    "        if used + tl <= token_budget:\n",
    "            acc.append(d.text); used += tl\n",
    "        else:\n",
    "            remain = token_budget - used\n",
    "            if remain > 50:\n",
    "                ids = llm_tokenizer(d.text, add_special_tokens=False)[\"input_ids\"][:remain]\n",
    "                acc.append(llm_tokenizer.decode(ids))\n",
    "            break\n",
    "    return \"\\n\\n\".join(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4e5fed-0733-47c3-bb2c-a4383b467e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e7b6d997d1471d9efcc8673ecc11a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/784 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773e9827388240829c9933aa820a6a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374822a2eda3489cb35a08fe411ab4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0b681e90864d969990b0fffa7b9f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf4fc7b1160445187595b463c188fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e3c94da3748c493f00ca8c8ca3f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df633336afad45eba7abf818557c7483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26146880c0204a5d8389643e167155a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2cdce413b34322a4de80e3d37251f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ce5251e98841a0b24a8ff3b82910b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x77aba91ae230>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- LLM 로드 & 생성 ----------------\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    try:\n",
    "        # llm_model.config.attn_implementation = \"sdpa\"\n",
    "        llm_model.config.attn_implementation = \"flash_attention_2\"\n",
    "        # llm_model.config.attn_implementation = \"eager\"\n",
    "    except Exception:\n",
    "        pass\n",
    "llm_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e61a59-701b-4c05-9af7-913f9622c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 텍스트 생성 ----------------\n",
    "def dynamic_max_new_tokens(question: str) -> int:\n",
    "    lines = [ln.strip() for ln in question.split(\"\\n\") if ln.strip()]\n",
    "    opt_cnt = sum(bool(re.match(r\"^\\d+(\\s|[.)])\", ln)) for ln in lines)\n",
    "    return 128 if opt_cnt >= 2 else 256\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 한국 법령, 금융, 보안 도메인 Q/A를 담당하는 도우미입니다. \"\n",
    "    \"아는 사실만 간결하게 답하고, 모르면 '알 수 없습니다'라고 말하세요.\"\n",
    ")\n",
    "USER_TPL = (\n",
    "    \"다음 컨텍스트만 사용해 한국어로 정확하게 답하세요.\\n\"\n",
    "    \"===\\n{context}\\n===\\n질문: {query}\"\n",
    ")\n",
    "\n",
    "def faiss_search_with_scores(query: str, top_k: int = TOP_K):\n",
    "    qv = np.array(embeddings.embed_query(query), dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(qv, top_k)              # L2 거리 (정규화 벡터)\n",
    "    cos = 1.0 - (D[0] / 2.0)                    # L2 -> cosine\n",
    "    out = []\n",
    "    for idx, i in enumerate(I[0]):\n",
    "        if int(i) >= 0:\n",
    "            out.append((docs[int(i)], float(cos[idx])))\n",
    "    return out\n",
    "\n",
    "def generate_answer(query: str) -> str:\n",
    "    # (0) 라우팅: 법/금융/보안 질의가 아니면 검색 자체를 생략 → 즉시 베이스모델\n",
    "    if not route_is_law(query):\n",
    "        prompt = (\n",
    "            \"당신은 한국어로 간결하고 정확하게 답하는 도우미입니다. \"\n",
    "            \"사실에 근거해 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048, padding=False)\n",
    "        inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "        with torch.inference_mode():\n",
    "            out = llm_model.generate(**inputs,\n",
    "                                     max_new_tokens=dynamic_max_new_tokens(query),\n",
    "                                     do_sample=False, temperature=0.2,\n",
    "                                     eos_token_id=llm_tokenizer.eos_token_id,\n",
    "                                     pad_token_id=llm_tokenizer.pad_token_id)\n",
    "        gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
    "\n",
    "    # (1) 벡터 검색 + 점수\n",
    "    scored = faiss_search_with_scores(query, top_k=TOP_K)\n",
    "    best_cos = max((s for _, s in scored), default=0.0)\n",
    "\n",
    "    # (2) 임계치(튜닝 포인트): 0.65 → 0.70로 올리면 컨텍스트 사용 빈도↓ → 평균 지연↓\n",
    "    THRESH = 0.70\n",
    "    use_context = best_cos >= THRESH and len(scored) > 0\n",
    "\n",
    "    # (3) 컨텍스트 조립 (캐시된 tok_len 사용)\n",
    "    ctx = pack_context([d for d, _ in scored], token_budget=CTX_TOKEN_BUDGET) if use_context else \"\"\n",
    "\n",
    "    # (4) 프롬프트 구성 (불필요한 장식 최소화)\n",
    "    if use_context:\n",
    "        prompt = (\n",
    "            \"아래 컨텍스트를 우선 사용해 정확히 답하세요. 불충분하면 아는 범위에서만 간결히 답하세요.\\n\\n\"\n",
    "            f\"=== 컨텍스트 ===\\n{ctx}\\n=== 끝 ===\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        max_new = dynamic_max_new_tokens(query)\n",
    "        max_len = 3072  # 입력 길이 상한도 줄여 토크나이즈 시간 단축\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"당신은 한국 법령·금융·보안 Q/A 도우미입니다. \"\n",
    "            \"사실에 근거해 간결히 답하고, 모르면 '알 수 없습니다'라고 말하세요.\\n\\n\"\n",
    "            f\"질문: {query}\"\n",
    "        )\n",
    "        max_new = dynamic_max_new_tokens(query)\n",
    "        max_len = 2048\n",
    "\n",
    "    # (5) 토크나이즈/생성\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n",
    "    inputs = {k: v.to(llm_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=False,\n",
    "            temperature=0.2,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        )\n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return llm_tokenizer.decode(gen, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a33789-82f3-44d7-a509-a6a1e8d2eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"개인정보보호법 제22조의2에 따라 만 14세 미만 아동의 개인정보를 처리하기 위해 필요한 절차로 옳은 것은?\n",
    "1 아동의 학교의 동의를 받아야 한다.\n",
    "2 법정대리인의 동의를 받아야 한다.\n",
    "3 아동 본인의 동의만 받으면 된다.\n",
    "4 아동의 친구의 동의를 받아야 한다.\"\"\"\n",
    "\n",
    "q = \"\"\"고려대학교에 대해서 설명해줘.\"\"\"\n",
    "\n",
    "\n",
    "print(generate_answer(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b8c76-b596-440f-8499-ec1a8cb1714d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cecdb-92da-46ba-a123-b264caa704e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722b58e-d526-402a-adc0-6a7bb64d6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b529261-8801-4767-b018-e6661f50f3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b13f45-4284-4eb9-99cf-ea904699ee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1267e-1633-4f42-b984-880c4e5f285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd50a0-f456-413c-ab49-1a662c48bff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12a9fc-cc4d-4390-b988-dde4a24ce55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44abf0a-0235-4eae-95db-1d2b458a7d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602aac4-9c52-4bbc-a961-1029572f1dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
